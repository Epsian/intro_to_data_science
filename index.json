[{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/r_rstudio/","title":"R and R Studio","tags":[],"description":"","content":" Overview Common Settings Overview R is a programming language and computing environment specialized for statistical analysis and data manipulation. It\u0026rsquo;s commonly used for performing statistical tests, creating data visualizations, and writing data analysis reports. Despite focusing on statistics, it\u0026rsquo;s a full-fledged programming language, and relatively easy to learn.\nYou should have gotten R and R studio install on the first data of SDS 100. If you did not, please follow the guide here.\nCommon Settings There are a few settings I recommend changing in R studio to make the process of working with it a little easier. In the top bar, click on Tools \u0026gt; Global Options and modify the following.\nUnder General \u0026gt; Basic \u0026gt; Workspace, disable \u0026ldquo;Restore .RData into workspace at startup.\u0026rdquo; Under General \u0026gt; Basic \u0026gt; Workspace, set \u0026ldquo;Save workspace to .RData on exit\u0026rdquo; to Never. Under Code \u0026gt; Editing, enable \u0026ldquo;Soft-wrap R source files.\u0026rdquo; Under Code \u0026gt; Display, enable \u0026ldquo;Show Margin\u0026rdquo; with \u0026ldquo;Margin Column\u0026rdquo; set to 80. Under Code \u0026gt; Display, enable \u0026ldquo;Highlight R Function Calls.\u0026rdquo; Under Code \u0026gt; Display, enable \u0026ldquo;Rainbow Parenthesis.\u0026rdquo; Under R Markdown \u0026gt; Basic, disable \u0026ldquo;Show output inline for all R Markdown documents.\u0026rdquo; Under R Markdown \u0026gt; Visual, disable \u0026ldquo;Use visual editor by default for new documents.\u0026rdquo; Under Appearance, pick a theme you like! [WIN ONLY] Under Terminal \u0026gt; General, set \u0026ldquo;New terminals open with\u0026rdquo; to \u0026ldquo;Bash\u0026rdquo; (You can only do this after you complete the install guide 7: Windows Subsystem for Linux) "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/projects/project_1/","title":"Smith College Museum of Art","tags":[],"description":"","content":" Overview Project Description Technical Details Tips for Success Overview The Smith College Museum of Art (SCMA) staff want to better understand how people use the museum website. Project 1 will focus on real SCMA data spanning from July 2021 to June 2022. Your goal is to create a portfolio of data visualizations which help communicate the usage patterns of the SCMA website to museum staff. You can think of this project as a formalized exploratory data analysis. At the end of Project 1, your visualizations and accompanying reports will be given to the museum staff for them to learn from!\nClick here for the Github Classroom Assignment for Project 1.\nProject Description Project 1 will be completed in self-organized teams of 3 to 4. Each member will be creating their own visualizations, but each visualization from the team will be combined into one portfolio. You should not expect the museum staff to be familiar with data, and should explain things in such a way that anyone could understand.\nAll of the data (aside from the shop data) are exported from the museum\u0026rsquo;s Google analytics platform. You can find the documentation for Google analytics here; you will need to use it to reference what some of the measures in the data mean. You do not need to use every data file; just use what is relevant to your exploration. You can choose to supplement the museum data with other data if you would like. The museum data files include the following:\ncountry_stats.csv: Website visit statistics by country (based on visitor IP address). country_users.csv: Website user statistics by country (based on visitor IP address). dates.csv: Visit statistics for the website by day. landing_page.csv: The website page which people \u0026ldquo;land\u0026rdquo; on the website, a.k.a. the first page they view. pages.csv: Statistics regarding the specific pages on the museum website. referrals.csv: What service visitors are coming from (where they click on the link to the museum website). search.csv: Statistics regarding terms search on the website. shop.csv: Data on sales from the museum shop. state_stats.csv: Website visit statistics by state (based on visitor IP address). state_users.csv: Website user statistics by state (based on visitor IP address). You can find the data files hosted on the course Moodle here. You should unzip the file, and place all of files contained inside the data/ folder within this project. You should not share these data files anywhere, including your project Github repo.\nTechnical Details You have 2 weeks (until start of class 10/28) to work on this project, including one full day of class time (10/14). You have full freedom to make the project as simple or intricate as you desire. Each member of the team must make at least one data visualization. You can also create tables or other summaries to help explain the data to the reader. Your final submission should include the following:\nIn your team Github repo:\nA Quarto portfolio document (any format) containing: All tables, summaries, and data visualizations you wish to present The code used to generate those items (in your code chunks set echo to true) A textual explanation of how you made those items for non-coders A textual explanation of what your summaries/tables/visualization mean A summary explanation of what lessons can be learned from your portfolio as a whole Through Moodle (Turn in here):\nA word pdf (there is a template in the docs folder) explaining your contributions to the project containing: A 1-2 paragraph summary of how you contributed to your team A list of each standard (per the course syllabus) you individually worked with in this project A justification for each standard describing what proficiency level you demonstrated per the text in the standards matrix The team portfolio should be created using Quarto in R studio. I recommend you output to a PDF if you have only static visualizations, or html if your visualizations are interactive; you can choose other formats if they work better for your project. You will commit your rendered report in the docs/ directory of this project.\nEach team will share one github repo. This repo should include all of your code. I recommend creating your visualizations in separate files, and then combining them in your Quarto portfolio document as a separate step.\nTips for Success Focus on creating a minimum viable product first. This means make something simple that satisfies all of the requirements, then go back and expand on what you have. Don\u0026rsquo;t try to create something ultra-fancy as your first milestone. Start by spending time understanding all of the variables in the provided data. Include a document in your project explaining what all the variables mean. Each person should work in a separate R script for each table/summary/visualization you want to create. Make combining them into your finalized portfolio a separate step. If there is a specific standard you want to raise, look for opportunities to do so. Each part of the data will have different challenges that you can overcome to show your growth. Don\u0026rsquo;t be afraid to branch out! There is no ceiling for this project. If you want to try a new tool or visualization style we haven\u0026rsquo;t covered in class, give it a go. You can explain how you think it shows your proficiency in your Moodle submission. "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/syllabus/","title":"Syllabus","tags":[],"description":"","content":" Course Description Course Structure Course Instructor Contacting Me Course Policies Required Materials Attendance Academic Honesty Code of Conduct Accommodation Grading Standards-Based Grading Standards Final Grades Late Work Policy FAQ Course Description Introduction to Data Science (SDS 192) aims to equip students with the knowledge and tools to understand, critically evaluate, manipulate, and explain data. This is an introductory course, and no prior experience is necessary1. Students will learn how to read and write code, but also how to create, organize, and collaborate on coding projects while critically examining the projects goals and data sources. We will be primarily using the R language, along with supplemental tools.\nCourse Structure Each week follows the same basic structure. Monday and Wednesday classes include lectures to introduce new concepts. Each lecture is followed by interactive problem sets designed to reinforce concepts through active learning. Slides from lecture will be posted online after class. The problem sets for any class are \u0026ldquo;due\u0026rdquo; at the start of the next class period when the answers will be released; most problem sets can be completed in class. In-class problem sets do not contribute toward your grade. They are intended to reinforce material and help you test your own understanding.\nFriday classes are devoted to lab activities or project work time. Students are expected to come to class for these activities. Labs include more involved problem sets that incorporate topics from the current and prior weeks. Students work on labs in groups of two to four people. Labs are reviewed through GitHub Classroom where feedback is provided.\nFor a full list of assignments and due dates, please see the course schedule.\nThis is a 4-credit course. You should be spending 12-hours total per week on this course. Expect to spend around 8.25 hours (12 hours - 3.75 hours/week of in-class instruction) on class material per week outside of class.\nCourse Instructor I am a sociologist that studies abuses of power in government. I earned my Ph.D.Â at the University of California, Davis in in sociology with a designated emphasis in computational social science. I combine computational methods such as social network analysis, natural language processing, geospatial analysis, and machine learning with open source and governmental data to uncover patterns of malfeasance and misfeasance by our public servants. From the political networks of politicians and prohibition gangsters to bias hidden in the text of academic recruitment, I use new methods to work on old problems of corruption and inequality.\nI am a visiting assistant professor in the Statistical \u0026amp; Data Sciences (SDS) program. I have experience working with both United States and United Kingdom governmental organizations applying machine learning to real-world problems. In the UK, I worked with the national lab for data science and machine learning, the Alan Turing Institute, on early-detection systems in foster care to assure children are receiving adequate services. Meanwhile in the US I worked with the Internal Revenue Service to build a machine learning system that determined the credibility of incoming fraud reports.\nContacting Me Slack Office Hours You can send me a message on the course Slack workspace, and I will respond when I am able, typically within 24 hours during the work week. To message me, click the + button next to \u0026ldquo;Direct Messages\u0026rdquo; and search for my name.\nIf your question is not sensitive in nature, consider putting it in the #coding-help or #course-help channel instead. There is a good chance one of your classmates will be able to answer before I can.\nSlack questions should be brief or administrative in nature. For more in-depth questions and troubleshooting please attend office hours.\nYou can schedule a meeting with me on Calendly. Drop-ins are welcome, but priority is given to those who make an appointment. Group appointments, to address a similar question, are welcome.\nIf you are coming to office hours with a coding question, make sure you have the code ready at the start of your appointment. Have your computer booted up and your project open.\nIf you cannot find an open time slot, please message me for an appointment. I will attempt to find a time that works for both of us.\nCourse Policies Required Materials Students are not expected to buy any materials for this course. Data science is built on free and open collaboration. There is no shortage of high-quality learning material available. This reader, as well as all assignments, are currently available for free.\nStudents are required to have a working computer (preferably a laptop) and reliable internet connection for this course. Any recent computer should be sufficient, with the notable exception of Chromebooks. Chromebooks lack access to the majority of the tools used by data scientists.\nIf you only have access to a Chromebook, please speak with me as soon as possible.\nAttendance I will not be taking attendance in this course, and you do not need to inform me when you will be absent. If you are sick, please stay home. Given the standards-based grading system (discussed below), no single class, assignment, or even quiz will negatively impact your grade. That said, it will be very difficult to keep up with course material without consistent attendance.\nIf you miss a class, you should contact a peer to discuss what was missed, and check the course reader website for any upcoming deadlines. I won\u0026rsquo;t have the capacity to re-deliver missed material in office hours.\nQuizzes cannot be made up after the open period has passed. If you have a known scheduling conflict with a quiz, please speak with me as soon as possible to arrange an alternative time.\nPlease see the SDS department\u0026rsquo;s official policy regarding remote learning:\nIn keeping with Smith\u0026rsquo;s core identity and mission as an in-person, residential college, the Program in Statistical \u0026amp; Data Sciences affirms College policy (as articulated by Provost Michael Thurston and Dean of the College Alex Keller) that students will attend class in person. As such, SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Office of Disability Services will be the only exceptions to this policy. As with any other kind of accommodations under the Americans with Disabilities Act (ADA), please notify your instructor during the first week of classes to schedule a meeting with them to discuss how we can work with you to provide the most accessible course possible.\nAcademic Honesty Data science is inherently collaborative, so I fully expect students to collaborate. You are encouraged to work together on most assignments\u0026mdash;ask questions on Slack, create study groups, and share helpful resources you find. However, anything you submit must be your own work. You need to be the person who writes the text and/or code. Multiple students should not submit identical work. Please note: The only avenue in which collaboration is not allowed is on quizzes.\nAll students, staff, and faculty are bound by the Smith College Honor Code:\nStudents and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information and honest and respectful use of college resources.\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. All submitted work of any kind must be the original work of the student who must cite all the sources used in its preparation.\n-Smith Academic Honor Code Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\nSubmitting work completed by another student as your own. Copying and pasting text or code from sources without quoting and citing the author. Paraphrasing material from another source without citing the author. Failing to cite your sources correctly. Falsifying or misrepresenting information in submitted work. Paying another student or service to complete assignments for you. Learning to code is similar to learning a new language; you will only learn by doing. No amount of rote copying will advance you beyond the most elementary levels of understanding. Please keep this in mind.\nIf someone else helps you understand a concept better, give them a nod in the #shoutouts channel on Slack.\nCode of Conduct As participants in this course we are committed to making participation a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned with this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant.\nAccommodation Smith College is committed to providing support services and reasonable accommodations to all students with disabilities. To request an accommodation, please register with the Office of Disability Services Office (ODS) at the beginning of the semester.\nGrading Standards-Based Grading This course will be graded using a standards-based grading system. Rather than tallying up the percentage of questions you answer correctly, I assess your responses by using a pre-defined set of course standards and then assign a level of proficiency. Throughout the semester, this course offers multiple opportunities to showcase the depth of your understanding in light of these standards.\nIn traditional points-style grading, an average is taken of all your assignments, and your final grade is based on that average. This means all assignments are given equal consideration in your final grade.\nMean of\nA1-A5 In contrast, standards-based grading is focused on your progression through the course. Functionally, only your best score for each standard is kept. All others are effectively forgotten. The hope is that without the worry of \u0026ldquo;getting a bad grade\u0026rdquo; when you are new to a concept, you will feel free to safely engage with complicated topics early on, make mistakes, and have opportunities to show improvement without penalization.\nMax of\nA1-A5 A standards-based grading system carries a number of other benefits:\nLearning targets for the course are clearly defined from the outset. Every graded assignment is directly tied to at least one standard. There is no \u0026ldquo;busy work\u0026rdquo; with a standards-based system. No single assignment will make-or-break your grade. You have multiple opportunities to demonstrate fluency in a standard. This rewards students that take the time to practice and learn from their mistakes. It prioritizes student growth throughout the course of the semester. Assessments in a standards-based system are much clearer than in a point-based grading system. Saying that I\u0026rsquo;ve become proficient in data wrangling, joining, and visualizing means more than saying that I earned a 92.5 in my Introduction to Data Science course. A standards-based grading system makes it easier to monitor your own progress towards a certain grade. There is no competition and no curve in a standards-based system. The only person you are ever compared with is your past self. Help each other often and freely. Standards The following table lists all the standards you are evaluated on in this course. There are 15 total standards, separated into 4 categories. Each standard states what conditions must be met to reach each proficiency level. There are four proficiency levels for each standard, each requiring more complete understanding of the material. These levels are inclusive, meaning to reach the 4th level, \u0026ldquo;Exceeds Standard\u0026rdquo; you must also meet all the requirements of level 3, \u0026ldquo;Meets Standard.\u0026rdquo;\nYou will have multiple opportunities to demonstrate your understanding of each standard. Any assignment that is reviewed is an opportunity to increase your proficiency level in a standard. In addition to the four levels of proficiency, there is also an extra point available in each standard called \u0026ldquo;Individual Standard.\u0026rdquo; You may fulfill this requirement only on quizzes, but only need to reach the \u0026ldquo;Meets Standard\u0026rdquo; criteria on a standard to do so.\nYou can demonstrate proficiency in any reviewed assignment, but can only fulfill the \u0026ldquo;Individual Standard\u0026rdquo; criteria on a quiz.\nStandard Does Not Meet Standard Progressing Toward Standard Meets Standard Exceeds Standard Individual Standard Data Importing Cannot import data or uses R Studio visual tools to import data. Manually organizes or modifies data before importing it into R. Can import raw data into R using the appropriate function for the data source. Can author API calls or use other remote sources and import data directly into R. Data Cleaning Cleans data in a non-programmatic way. Can clean data programmatically on a cell-by-cell basis to prepare it for analysis. Can assign the correct common data types (logical, integer, numeric, factor, and string) to loaded data and understand the uses of each. Can clean data for analysis in a vectorized way. Can prepare data for advanced types (dates, time series, etc.). Can prepare data from non-traditional sources such as OCR or web scraping. Data Reshaping Formats data in a non-programmatic way. Can derive new measures from existing data and append it to dataframes. Can pivot data between wide and long formats, and can explain the use case of each. Can transition data between data frames and lists, and explain the applications of data in a list format. Data Aggregation \u0026amp; Subsetting Transforms data in a non-programmatic way. Creates multiple copies of data in several intermediate stages of transformation that are used for different steps of analysis. Can combine and split data sets using the appropriate merge or subset techniques. Can split or merge data sets using either SQL-like calls or approximate matching. Functions Copies-and-pastes similar code with small changes. Creates simple functions with consistent inputs. Creates simple functions that can handle novel inputs, with logic to handle the data appropriately. Creates complex functions that can handle arbitrary input. Includes built in error checking and warnings. Iteration Copies-and-pastes similar code several times within or between scripts. Uses for loops or apply functions to iterate through vector data to preform a single data manipulation. Can use either loops or apply functions to iterate over a vector of data and preform multi-step manipulations. Can use loops or apply functions and explain the use cases for each. Can iterate over complex data structures such as dataframes or lists. Visualization Structure Selects inappropriate formats for data visualization. Selects sub-optimal visualization formats or uses excessive visualizations where a single one would be sufficient. Selects suitable formats for data visualization (bar, line, boxplot, etc.) and can explain the reasoning behind that choice. Effectively mixes visualization formats or isolates individual elements to clearly communicate a message. Visualization Aesthetics Chooses visual cues and colors for purely aesthetic reasons without attention to data representation. Data visualizations attempt to represent underlying data, but use methods unsuited to the task which leave ambiguity for the viewer. Data visualizations use color, scale, and shapes effectively to differentiate and communicate underlying data. Data visualizations are highly customized with bespoke elements, such as callouts, to clearly communicate the message of the visualization. Aesthetics are sensitive to accessibility concerns. Visualization Context Produces data visualizations that are unclear, confusing, devoid of context, or impossible to understand without reading the text. Produces data visualizations with readable axis labels, units, and legends (where appropriate). Produces data visualizations that are clear and understandable with minimal textual explanation. Produces data visualizations that are self-contained and can be understood on their own without textual explanation. Data Ethics Does not consider data ethics or investigate data provenance. Can articulate common pitfalls and relate them to the project at hand. Confirms data types and scales using data documentation. Reads data documentation to understand data collection/generation and measurements. Can highlight potential concerns specific to the data or project. Either creates data documentation for used data, or includes notes in code to the data sources and explains potential pitfalls. Considers and articulates relevant concerns related to the current project unprompted throughout the work cycle. Code Style Code style is inconsistent and/or lacks documentation. Code comments explain the broad strokes of intended behavior. Indentation is consistent and predictable. Uses print statements to track the status of code execution. Consistently comments all code with clearly organized sections. Expected inputs and outputs are clearly explained. Uses error messages or print statements within their code to locate the causes of errors and resolve them. Code is clearly commented, with standardized formatting and indentation. Code contains tests which will check for errors, and report those errors if they arise. Git/Github Does not use git for version control. Uses git and GitHub for version control and can contribute to group repositories with commits, pushes, and pulls. Uses git and GitHub effectively. Code commits are of appropriate size and commented well. Can branch and merge repositories while resolving any merge conflicts. Does not include sensitive files in commits. Uses Github effectively for collaboration. Can create issues, ask for review, and merge branches in a manner suitable for a collaborative environment. Final Grades Your completion of these standards are converted into a final letter grade using the following process. Each of the 12 standards will be converted into a four-point scale, with one point available for meeting the \u0026ldquo;Individual Standard\u0026rdquo; on a quiz.\n1 Point. \u0026ldquo;Does Not Meet Standard\u0026rdquo; 2 Points. \u0026ldquo;Progressing Toward Standard\u0026rdquo; 3 Points. \u0026ldquo;Meets Standard\u0026rdquo; 4 Points. \u0026ldquo;Exceeds Standard\u0026rdquo; +1 Point. \u0026ldquo;Individual Standard\u0026rdquo; On this scale, there are 60 points total in the course (12 standards * 5 possible points). I sum the highest level of proficiency you reach in each standard over the course of the semester to arrive at your final score. For example, if someone were to reach \u0026ldquo;Exceeds Standard\u0026rdquo; in all standards, but could never do so on a quiz, they would receive 48 of 60 points (4 points * 12 standards). Similarly, if someone reaches \u0026ldquo;Meets Standard\u0026rdquo; in all topics, including on quizzes, but did not reach \u0026ldquo;Exceeds Standard\u0026rdquo; in any topic, they would likewise receive 48 of 60 points.\nThe summed points will be converted into letter grades using the following table.\nLetter Points A 57-60 A- 54-56 B+ 52-53 B 50-51 B- 48-49 C+ 46-47 C 44-45 C- 42-43 D+ 40-41 D 38-39 D- 36-37 F 0-35 Late Work Policy Assignments turned in late will not be reviewed, and will not be considered for demonstrating proficiency in course standards. Keep in mind, missing an assignment will not hurt your grade, but does remove one chance for you to demonstrate your knowledge of course material. If you do not think you will be able to turn in an assignment by the deadline, you may request an extension. To do so, please send me a message explaining why you are unable to complete the assignment in the expected time frame. Extension requests must be made\u0026ndash;and accepted\u0026ndash;before the assignment due date.\nAfter the due date, late assignments are only reviewed if there are emergency circumstances preventing you from turning the assignment in on time.\nFAQ Q: So if I reach \u0026ldquo;Exceeds Standard\u0026rdquo; and fulfill the individual standard on a quiz for a topic early in the semester, I can just skip those questions for the rest of the class?\nA: Theoretically yes, but I would recommend you answer all questions to make sure you\u0026rsquo;re not letting your knowledge slip.\nIf this is your first course in the SDS department, you also need to enroll in SDS 100.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/git/","title":"git","tags":[],"description":"","content":" Overview git on Windows git on Mac Overview git is a tool for version control and collaboration. It is the tool used by data science teams big and small to keep track of code. Think of it like track changes in Word or Google docs, but for code files.\nYou will also need an account on Github. Please create one here.\ngit on Windows Follow these step-by-step instructions if you\u0026rsquo;re installing Git on a Windows machine:\nFirst, launch a web browser, the image below shows the Microsoft Edge browser.\nNext, navigate to the following Git download URL in your browser https://git-scm/com/downloads.\nSelect \u0026ldquo;Windows\u0026rdquo; from the Downloads portion of the Git webpage. Git will display the following page and automatically being downloading the correct version of the Git software. If the download doesn\u0026rsquo;t start automatically, click on the \u0026ldquo;Click here to download manually link.\u0026rdquo;\nWhen the download is complete, open/Run the downloaded file (it may look different in different browsers).\nA screen will appear asking for permissions for the Git application to make changes to your device. Click on the Yes button.\nClick Next to accept the user license.\nLeave the default \u0026ldquo;Destination Location\u0026rdquo; unchanged (usually C:\\Program Files\\Git) and hit Next\nYou will see a screen like the one below asking you to \u0026ldquo;Select Components.\u0026rdquo; Leave all of the default components selected. You can also check the boxes next to \u0026ldquo;Additional Icons\u0026rdquo; and it\u0026rsquo;s sub-item, \u0026ldquo;On the Desktop\u0026rdquo; if you would like. Your completed configurations window should have the following components selected:\nAdditional Icons -\u0026gt; On the Desktop Windows Explorer integration -\u0026gt; Git Bash Here -\u0026gt; Git GUI Here Git LFS (Large File Support) Associate .git* configuration files with default text editor Associate .sh files to be run with Bash The next screen will ask you to pick a \u0026ldquo;default editor, click the drop down box and select\u0026quot;Use the Nano editor by default.\u0026rdquo; The press Next.\nOn the next screen, it will ask to override the default \u0026ldquo;branch name.\u0026rdquo; Select the \u0026ldquo;Override the default branch name for new repositories\u0026rdquo; option, and in the text box type \u0026ldquo;main.\u0026rdquo; Press Next.\nThe next screen will ask you if you want to adjust your path environment. Leave the default of \u0026ldquo;Git from the command line and also from 3rd-party software.\u0026rdquo; Press Next.\nOn the next screen, keep the default option of \u0026ldquo;Use bundled OpenSSH.\u0026rdquo; Press Next\nOn the next screen, keep the default option of \u0026ldquo;Use the OpenSSL library.\u0026rdquo; Press Next.\nLeave the default \u0026ldquo;Checkout Windows-style, commit Unix-style line endings\u0026rdquo; selected on the next page and hit Next:\nKeep the default \u0026ldquo;Use MinTTY (the default terminal of MSYS2)\u0026rdquo; selected on the \u0026ldquo;Configuring the terminal emulator to use with Git Bash\u0026rdquo; window and hit Next:\nKeep the default value of \u0026ldquo;Default (fast-forward or merge)\u0026rdquo; on the \u0026ldquo;Choose the default behavior of \u0026lsquo;git pull\u0026rsquo;\u0026rdquo; page and hit Next:\nKeep the default value of \u0026ldquo;Git Credential Manager Core\u0026rdquo; on the \u0026ldquo;Choose a credential helper\u0026rdquo; page and hit Next:\nKeep the default values on the \u0026ldquo;Configuration extra options\u0026rdquo; page by keeping \u0026ldquo;Enable file system caching\u0026rdquo; checked and \u0026ldquo;Enable symbolic links\u0026rdquo; unchecked and then hit Next:\nMake sure that no options are checked in the \u0026ldquo;Configuring experimental options\u0026rdquo; page and hit Install:\nAfter you hit this Install button as per above, you will see an install progress screen like the one below:\nWhen the install is complete, a new, \u0026ldquo;Completing the Git Setup Wizard\u0026rdquo; window like the one below will appear:\nMake sure that all of the options on this window are unchecked as in the image below and then hit the Finish button:\nThis will complete your installation process. Type in git --version to check if everything was installed correctly. If you see git version \u0026lt;NUMBERS\u0026gt; you\u0026rsquo;re all set. Now we need to configure some settings. Right click on your desktop, and click on \u0026ldquo;Git Bash here.\u0026rdquo; A black terminal window will open.\nClick on the window, and then copy the following and press enter, changing \u0026ldquo;Jane Doe\u0026rdquo; to your name. You must put your name in quotes. git config --global user.name \u0026quot;Jane Doe\u0026quot;\nLastly, copy the following and press enter, changing the email to your email address. git config --global user.email jdoe@example.com\ngit on Mac To install git on a Mac, first open the launchpad by pressing F4 or by making a pinch motion on the track pad with three fingers and your thumb.\nA terminal window will open up, showing your account name and then a $, with a flashing cursor afterwards. You will enter text here to issue commands.\nEnter type in the word git and press enter.\nA window will pop up, asking if you want to install \u0026ldquo;developer tools.\u0026rdquo; Click Install.\nA prompt will appear asking you to agree to the license agreement, click Agree.\nThe software will then start installing. It will take a few minutes to finish. When it is done you will see the following window. Click Done.\nTo make sure everything is installed correctly, go back to the terminal window and enter git --version. You should see a message that says git version \u0026lt;NUMBERS\u0026gt;. If you do, you can move on.\nNext we will need to set up some options. Fist, copy the following into the terminal and press enter to change the default branch name: git config --global init.defaultBranch main\nNext, copy the following and press enter, changing \u0026ldquo;Jane Doe\u0026rdquo; to your name. You must put your name in quotes. git config --global user.name \u0026quot;Jane Doe\u0026quot;\nLastly, copy the following and press enter, changing the email to your email address. git config --global user.email jdoe@example.com\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/","title":"Install Guides","tags":[],"description":"","content":" R and R Studio Overview Common Settings Overview R is a programming language and computing environment specialized for statistical analysis and data manipulation. It\u0026rsquo;s commonly used for performing statistical tests, creating data visualizations, and writing data analysis reports. Despite focusing on statistics, it\u0026rsquo;s a full-fledged programming language, and relatively easy to learn. You should have gotten R and R studio install on the first data of SDS 100. If you did not, please follow the guide here.\ngit Overview git on Windows git on Mac Overview git is a tool for version control and collaboration. It is the tool used by data science teams big and small to keep track of code. Think of it like track changes in Word or Google docs, but for code files. You will also need an account on Github. Please create one here. git on Windows Follow these step-by-step instructions if you\u0026rsquo;re installing Git on a Windows machine:\nGithub Overview Create an Account Creating SSH Keys and Adding to Github Overview Github is a online code repository that great expands the utility of git. It acts as a clearinghouse for code, and is used worldwide by researchers, government, and industry. Create an Account First up, we need to create an account on github.com. Navigate to the site, and click the Sign up button in the upper right. Enter your email and create a password.\nDB rowser Overview DB Browser on Windows DB Browser on Mac Overview DB Browser is an ultra lightweight viewer for SQLite databases. It is made to allow those familiar with spreadsheets to work more easily with the common SQLite format. However, that is all that it does; you cannot use it on other database types. DB Browser on Windows First head to the DB Browser download page, and select the version that matches your system.\nOpenRefine Overview OpenRefine on Windows OpenRefine on Mac Overview OpenRefine is an open source tool used to clean and pre-process messy data. While most people are familiar with data cleaning in their coding tool of choice (R, Python, Julia, etc.), OpenRefine is designed to provide powerful cleaning capabilities with minimal overhead. One of the most helpful capabilities of OpenRefine is the ability to check for possible duplicates and misspellings of text data using it\u0026rsquo;s text facet tools.\nR Packages Overview Overview R uses a number of packages to work with data, which are largely community created. This means many of them do not come pre-installed with R. Here is a list of packages we will use this semester. You should be able to paste this into the R console and press enter to install them all at once. install.packages(\u0026quot;tidyverse\u0026quot;); install.packages(\u0026quot;dplyr\u0026quot;); install.packages(\u0026quot;skimr\u0026quot;); install.packages(\u0026quot;ggplot2\u0026quot;); install.packages(\u0026quot;mosaic\u0026quot;); install.packages(\u0026quot;plotly\u0026quot;); install.packages(\u0026quot;todor\u0026quot;); install.packages(\u0026quot;compareDF\u0026quot;); install.packages(\u0026quot;future\u0026quot;); install.packages(\u0026quot;rvest\u0026quot;); install.\n[WIN ONLY] Windows Subsystem for Linux Overview Check your windows version Windows version larger than (or equal to) 19041 Windows version smaller than 19041 Verifying your install Overview Windows now has the ability to install a linux operating system on your machine without the use of an emulator. This gives you a full-featured linux environment that can interact with your normal files. Check your windows version First, please check the build version of Windows that you are using.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/github/","title":"Github","tags":[],"description":"","content":" Overview Create an Account Creating SSH Keys and Adding to Github Overview Github is a online code repository that great expands the utility of git. It acts as a clearinghouse for code, and is used worldwide by researchers, government, and industry.\nCreate an Account First up, we need to create an account on github.com. Navigate to the site, and click the Sign up button in the upper right.\nEnter your email and create a password.\nYou will most likely receive an email from Github asking to confirm your account. Go and click that.\nCreating SSH Keys and Adding to Github Creating a new SSH key will invalidate all the places your current SSH key is used!\nSSH Keys are a way to identify your computer when accessing external resources. Think about it like a password for your computer to log in by itself. The first thing we need to do is create an SSH key for your computer. Open up R Studio, and click on the Terminal tab in the bottom left pane. Copy the following, enter your correct email, and press enter to create a key: ssh-keygen -t ed25519 -C \u0026quot;your_email@example.com\u0026quot;\nThe terminal will look slightly different in my pictures, but the process is the same in the R Studio terminal.\nIt will ask where you want to save the key. Accept the defaults by pressing Enter.\nIt will then ask you to create a pass phrase, press enter twice to skip this step.\nIt will then show a printout of your key, and a little bit of art. I\u0026rsquo;ve greyed mine out here for security.\nNext, type cd and press enter, followed by cat .ssh/id_ed25519.pub. It will print out a code starting with \u0026ldquo;ssh-ed25519 \u0026hellip; your_email@smith.edu.\u0026rdquo; You want to highlight all of that, including the \u0026ldquo;ssh-ed25519\u0026rdquo; and your email, press CTRL or command + C to copy it.\nWe will now return to github.com and login.\nIn the upper right hand corner you will see your user profile dropdown. Click on that and go to Settings.\nOn your setting screen, in the left hand menu, click on \u0026ldquo;SSH and GPG keys.\u0026rdquo;\nOn the next screen, click the green button in the upper right that says \u0026ldquo;New SSH Key.\u0026rdquo;\nOn the following screen, name your key, and paste the text we copied into the \u0026ldquo;Key\u0026rdquo; box. Then press the \u0026ldquo;Add SSH Key\u0026rdquo; button.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/","title":"Worksheets","tags":[],"description":"","content":" R/R studio Worksheet Practise some R fundamentals.\ngit Worksheet Practise some git fundamentals.\nExploratory data analyses Worksheet Intro to exploratory data analyses.\nTidy Data Worksheet An intro to tidy data and pivoting.\nAggregation and Merging Worksheet How to aggregate within datasets and merge across them.\nAdvanced Plotting How to make our plots actually look nice.\nDynamic Plotting How to elevate your plots.\nData Science Ethics Tips for not creating a monster.\nFunctions Create your own tools in R!\nDebugging and Conditions Tools to fix errors and test conditions.\nIteration Iterate through elements and give our function superpowers.\nList and Apply The list data type and apply family of functions.\nBash Learn to navigate the shell by playing a game!\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/dbbrowser/","title":"DB rowser","tags":[],"description":"","content":" Overview DB Browser on Windows DB Browser on Mac Overview DB Browser is an ultra lightweight viewer for SQLite databases. It is made to allow those familiar with spreadsheets to work more easily with the common SQLite format. However, that is all that it does; you cannot use it on other database types.\nDB Browser on Windows First head to the DB Browser download page, and select the version that matches your system. On Windows this is most likely the 64-bit standard installer.\nOnce the download has finished, start the installer from the browser or your download location.\nOnce you have started the installer, press the Next button to continue.\nOn the next screen, accept the license agreement and press Next. The following page will ask you if you would like to place shortcuts anywhere. Desktop will place icons on your desktop, while Program Menu will add options to your right click menu. You can choose to have these if you wish, I will not. Press Next once you have decided.\nOn the following screen, you will be asked to select what components you want to install and where. You can safely keep the defaults and continue. Press Install on the following page.\nOne the installation is complete, you can press Finish to close the installer!\nDB Browser on Mac First head to the DB Browser download page, and select the version that matches your system. There should only be one for Macs.\nOnce the download has finished, launch the installer. You will most likely see an alert from your browser.\nTo finish installing, drag the DB Browser icon into your apps folder.\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/labs/","title":"Labs","tags":[],"description":"","content":"All labs will be turned in through Github Classroom. You will set up your student account in the process on completing your first lab. Click the link below to open the lab.\nLab 0: Problem Solving Mini-Lab Lab 1: R Coding Lab 2: Exploratory Data Analyses (EDA) Lab 3: Tidy Data, Aggregation, and Merging Lab 4: Advanced Plotting Lab 5: Functions and Conditions Lab 6: Loops and Apply "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/4_r_rstudio/","title":"R/R studio Worksheet","tags":[],"description":"","content":" Overview Problem Sets 1. Data Types 2. Vectors 3. Objects 4. NAs 5. Dataframes 6. Conditionals Overview Learning to code is like learning a language, meaning you can only get better by practicing. Today we\u0026rsquo;re going to practice some R fundamentals. You are encouraged to work with your neighbors on these problems. If you ever get stuck, call over myself or the data assistant for help.\nProblem Sets 1. Data Types There are five main data types in R, they are:\nlogical - TRUE or FALSE integer - whole numbers like 1, 5, 100 numeric - numbers with decimal places like 5.25 character - anything with letters factors - for categorical data, numbers with descriptive labels NA - NA are the absence of data, or something missing. Depending on the type of data, you can only perform certain actions on them. For example, in the console, type 1 + 4 and press enter. Works fine! Now try 1 + \u0026quot;pie\u0026quot;. No good. In the later example, we tried to add a numeric and a character, which R can\u0026rsquo;t understand.\nFor each of the following, try using the class() function to discover the data type. Try to guess what each one is before you run the function.\n5 6.4 TRUE \u0026quot;FALSE\u0026quot; 2. Vectors Everything in R is a vector, which is an ordered arrangement of data of the same type. Even single bit of data, like if you enter \u0026quot;a\u0026quot; into the console, returns [1] \u0026quot;a\u0026quot;, which is showing a vector of length 1, with our data \u0026quot;a\u0026quot; in it. You can test it out yourself using the length() function.\nIn the console, type length(\u0026quot;a\u0026quot;) and press enter. The length() function will look at a vector, and tell you how many elements it has, or how many pieces of data there are inside it. As an example, try the following; in the console type c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), it returns a vector with three elements. Now try length(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;)), what did it return? Notice that is only returned a single number describing the vector it was given.\nYou can also run class() on a vector, which is an ordered arrangement of data. you can make vectors using the c(), or combine, function. Try to guess the type of the following vectors before using class(). Remember, a vector can only hold one type of data!\nc(1, 2, 3, 4) c(\u0026quot;five\u0026quot;, \u0026quot;six\u0026quot;, \u0026quot;seven\u0026quot;) c(8, \u0026quot;9\u0026quot;, 10) c(TRUE, FALSE) Vectors take some getting used to, but as you come to understand how they work, you will be able to take advantage of how powerful R really is. One important property of a vector is that you can apply actions to all elements of a vector at once. For example, once again run 1 + 4 in the console. You will get back a vector of length 1, with the result of [1] 5, meaning the element in position 1 of our vector is 5. Now try 1 + c(4, 5, 6, 7). What do you think will happen?\nThis vectorized mode of thinking takes some time to get used to. Run the following after trying to anticipate the results:\n5 + c(10, 20, 30) c(10, 50, 100) / 5 c(1, 5, 10) + c(2, 4, 8) Really take the time to think over these results, especially the third one.\nExplain the results of 5 + c(10, 20, 30) and c(1, 5, 10) + c(2, 4, 8). What is being done to each vector that would cause the results we are seeing?\nIn the first example (5 + c(10, 20, 30)) five is being added to each element of the vector.\nIn the second example (c(1, 5, 10) + c(2, 4, 8)) each element of the vector is being added with the element in the same position of the other vector.\nSome functionalities of R only make sense on a vector, for example, taking the mean() or average. It would be pointless to take the average of one number!\nTry this:\nmean(c(1, 5, 10))\n3. Objects You don\u0026rsquo;t have to type out your vector every time you want to use it, you can save it to an object using an assignment. Try typing letter_vec \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) and pressing enter. You should see letter_vec appear in your environment tab on the upper right. Type letter_vec into the console and press enter. We see the same data as if we had just entered c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) again, because that is what we saved inside the letter_vec object.\nCreate a vector called number_vec of 10 numbers. The numbers can be any you like.\nnumber_vec \u0026lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nWe can now start to do things to the object we created. Try using class() and length() on our letter_vec and number_vec objects.\nYou can use vectors with each other as well. Try the following: number_vec + number_vec\nYou can ask for only part of a vector by using the square brackets [ ]. Say we wanted the letters in the first, third, and fifth positions of our letter_vec object. I could ask R for letter_vec[c(1, 3)], and get back c(\u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;).\nCreate a vector called example_vector containing c(1, 3, 5, 7, 9, 11), and then get the values in the fourth, fifth, and sixth position.\nexample_vector \u0026lt;- c(1, 3, 5, 7, 9, 11)\nexample_vector[c(4, 5, 6)]\n4. NAs NAs are missing values. They cause a lot of problems, and are very common. A good portion of the work of data scientists is know how to work around missing values. Let\u0026rsquo;s re-do some of our earlier examples to to get a handle on NAs.\nFirst, what happens if we try and add to a NA?\nNA + 1\nIt will return another NA. Why? Because we don\u0026rsquo;t know what the value of 1 plus an unknown would be, so it is safest to just remain unknown. The same is true with NAs in a vector.\n1 + c(NA, 1, 2)\nWe can get the results for known values, but the unknown will stay unknown. This is true if something uses all elements of a vector, like mean(). Try: mean(c(NA, 5, 10)). Everything turns into an NA, because without knowing all the numbers, we can\u0026rsquo;t be certain what the mean would be.\nNAs do count as an element in a vector though, as we can see if we run length(c(1, 2, 3)) and length(c(NA, 2, 3)). Put simply, NAs exist, we just don\u0026rsquo;t know what they are.\nImagine you are collecting data by asking people questions on a survey. When would you want to write down NA? Is there a difference between NA and someone responding \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo;?\nAn NA is meant to represent missing data. For example, if you handed out a survey, and someone returned it to you without an answer. If someone wrote \u0026lsquo;I don\u0026rsquo;t know\u0026rsquo; that is an answer in itself, and thus not an NA.\n5. Dataframes Recall that dataframes are aggregations of vectors into rows and columns. They must be square, meaning that all the columns must be of equal length, and all rows must have values in every column. There can be NAs though, so watch out! For this section, we\u0026rsquo;re going to be looking at the results of our class survey.\nTo get started, copy the following into the R console and press enter to run it. read.csv is a function that reads tabular (square) data into R and creates a dataframe so we can work with it. We are giving it the argument of the URL for our class data survey to load that data from. We are then asking it to put the results into an object called survey. We will be coming back to this data later on in the problem set.\nYou can learn more about read.csv by opening the help file using ?read.csv.\nsurvey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) Exploration Let\u0026rsquo;s start by looking at the survey. You can do this by going over to the Environment tab in the upper right pane in R Studio. Click on the survey object you see there. This will open a viewer for you to look at the data. Can you find yourself?\nWe can also use some functions to get a summary. Start by entering str(survey) into the console. str() stands for Structure, and gives us an overview of dataframe objects. We can see how many obs or observations there are (rows) as well as how many variables there are (columns). It then lists all the variables, what type they are, and a preview of the contents.\nHow many variables are there of each type in the survey dataframe?\nLogical: 6 Integer: 3 Numeric: 0 Character: 14 Another good tool for starting to understand a dataframe is the head() function. head() will display the first handful of rows for you in the console so you can see what they look like. There is also the companion function of tail(), but that is used less often. Try both on our survey data before moving on.\nSubsetting One of the most important skills you will develop in R is how to work with dataframes, and how to subset them, or select only the content from them that you want. The two basic ways to do this is with the dollar sign $ and with square brackets [ ]. The $ lets you ask for specific columns from a dataframe. For example, if I wanted the column of just the average hours of sleep from our class, I could call ask for the hours_sleep column from the survey dataframe by entering survey$hours_sleep. Try that now. You should get back a vector of that single column. The same format will work to call any column by name.\nUsing the $ and the mean(), find the average hours of sleep our class gets per night.\nmean(survey$hours_sleep)\nAnother way to subset dataframes with with square brackets [ ]. Imaging a dataframe is like a map with a grid. You can find any spot on that map by finding the intersection of the grid coordinates. The same is true of a dataframe. For example, if I wanted the value in the third row and fourth column in our survey dataframe (it contains the value 27), I could ask for it by entering survey[*row*, *column*] or survey[3, 4].\nGet the values of each of the following:\nRow 5, column 5 Row 2, column 20 Row 13, column 11 survey[5,5] = r survey[5,5] survey[2,20] = r survey[2,20] survey[13,11] = r survey[13,11] You can also use the square brackets to ask for full columns, like $. Simply put the name of the column (in quotes!) in place of the column number, for example survey[ , \u0026quot;hours_sleep\u0026quot;]. Whenever you want everything, you can just leave a blank space. You can even get whole rows this way, but that isn\u0026rsquo;t used as often; like this survey[1, ].\nA good way to work out how to use square brackets is the phrase \u0026ldquo;such that.\u0026rdquo; For example, if I wanted \u0026ldquo;survey data such that it included rows 1, 2, and 5, and column \u0026lsquo;hours sleep,\u0026rsquo;\u0026rdquo; that would translate to survey[c(1, 2, 5), \u0026quot;hours_sleep\u0026quot;].\nAlways use the names of columns when using square brackets if possible. Columns may move what spot they are in, so calling them by number position can be dangerous. R will return whatever is in that position, regardless of what you want! But it will always find the column with the same name, no matter what spot it is in.\nAdding to Dataframes You can add to dataframes using the same tools to subset from them. Rather than describing where to take data from, you\u0026rsquo;ll now be describing where the new data goes.\nSay we wanted to add a new column to our survey dataframe. First, we would need some new data to add that is of equal length to the number of cases (rows) in the dataframe. We have 15 rows, so we need a vector of data of the same length, so one value will fit into each row of the dataframe. Let\u0026rsquo;s make a new vector of letters of the proper length.\nletters is a pre-built object in R, meaning it is always there in the background; you can see it by typing letters into the console. Let\u0026rsquo;s use our new sub-setting skills to get enough letters to fit our dataframe. We can do this by creating a new object, new_column_vector, and assigning the appropriate number of letters, in our case 15; we can even let R pick the right number using the nrow() function! Try this: new_column_vector \u0026lt;- letters[1:nrow(survey)]. You may have noticed a shortcut I used here. If I want some numbers that are all next to each other, like 1-2-3 or 9-10-11, I can use a colon : to say \u0026ldquo;from this number to this number.\u0026rdquo; So 1:10 is the same thing as c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); handy!\nNow that we\u0026rsquo;ve got our vector, we can add it to our dataframe! All you have to do is tell R what you want the new column to be called, and what to put in it. It looks like this: survey$COLUMN_NAME \u0026lt;- STUFF_TO_PUT_IN_COLUMN.\nPut our new_column_vector in our survey dataframe, calling the new column alphabet.\nsurvey$alphabet \u0026lt;- new_column_vector\nNow if we inspect out dataframe by clicking on it in the environment pane, we should be able to scroll all the way to the right and see our new column!\n6. Conditionals Another way to subset is by making comparisons. Say we wanted to see the rows in our data only for people whose birthday is in May. If we go look at our survey data in the viewer by clicking on it in the environment pane again, we can see the variable b_month has months in it. Let\u0026rsquo;s test the type of that column using class(survey$b_month)\nNow that we know the type of the b_month column, we can use that to subset the data. Where before we were just taking whole sections of the dataframe, now we are going to be asking for specific parts that match certain conditions, thus this is called conditional sub-setting. We define these conditions using comparison operators.\nComparison For now, we want to get the data only for those rows such that b_month is May. We can do that using the double equal sign comparison operator in R, ==. Comparison operators all compare one thing against another, and tell you if that comparison is TRUE or FALSE. The Equal operator, ==, will test if one thing, or vector of things, is equal to another. For example, try executing the following in the console:\n1 == 1 1 == 2 1 == c(1, 2, 1) Now, let\u0026rsquo;s apply that same idea to our b_month column. Try executing survey$b_month == \u0026quot;May\u0026quot;. This will take our b_month column, and test the condition that the values in b_month are equal to \u0026ldquo;May.\u0026rdquo; Note that capitalization matters! This get\u0026rsquo;s us halfway to our goal of seeing the data only for rows where b_month is May, but it\u0026rsquo;s not quite what we want. We now have a vector of TRUE and FALSE for b_month, now we want to use that to actually subset our data.\nWe are going to ask R for our survey data, such that we only see the rows there b_month is equal to May, including all columns. We can do that using survey[survey$b_month == \u0026quot;May\u0026quot;, ].\nThere are several other common comparison operators, one of the most important being !=, which stands for not equal to. Try running survey[survey$b_month != \u0026quot;May\u0026quot;, ], what does it return? Do you understand why?\nThe most common conditionals include:\n== - Equal to != - Not equal to \u0026gt; - Greater than \u0026gt;= - Greater than or equal to \u0026lt; - Less than \u0026lt;= - Less than or equal to "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/open_refine/","title":"OpenRefine","tags":[],"description":"","content":" Overview OpenRefine on Windows OpenRefine on Mac Overview OpenRefine is an open source tool used to clean and pre-process messy data. While most people are familiar with data cleaning in their coding tool of choice (R, Python, Julia, etc.), OpenRefine is designed to provide powerful cleaning capabilities with minimal overhead. One of the most helpful capabilities of OpenRefine is the ability to check for possible duplicates and misspellings of text data using it\u0026rsquo;s text facet tools.\nOpenRefine on Windows Open your web browser of choice and navigate to the OpenRefine homepage at https://openrefine.org/. Click on the download button in the left sidebar.\nOn the download page, scroll to the latest version of OpenRefine and select the Windows kit. If you are unsure if you have Java installed on your system, choose the Windows kit with embedded Java instead.\nOnce the download has completed, open the zip and move the contents to a convenient location on your computer.\nOpen the resulting directory, and double click on the openrefine.exe executable.\nThe OpenRefine executable will start a terminal window, and shortly after launch a tab in your default web browser with the OpenRefine interface.\nOpenRefine on Mac First, head to the download page for OpenRefine and choose the latest version for mac.\nOnce the townload has finished, open the downloaded file. Your borwser will most likely show an alert.\nOpen your Applications folder in the finder, and drag OpenRefine into the folder.\nOnce you have dragged the application into the Applications folder, try to open it. If you receive an alert like the following, continue to the next step.\nHold down the Control key and click on OpenRefine. Click open in the menu.\nIt will give you an option to open OpenRefine. Click Open.\nIt will ask if you want OpenRefine to control Safari and access your files. Click OK.\nA safari window will then open, and should look like the following. If that is the case you are all done!\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/projects/","title":"Projects","tags":[],"description":"","content":" Smith College Museum of Art Project 1: Exploratory data analyses for the Smith College Museum of Art.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/6_git/","title":"git Worksheet","tags":[],"description":"","content":" Overview Problem Sets 1. Make a New Repository 2. Your First Commit 3. Making a Change 4. Adding More Files 5. Breaking Things 6. Time Travel for Beginners 7. To the Cloud 8. Conflict Overview git is remarkably useful, but it takes an investment in using it well to gain the full benefits. Today we\u0026rsquo;re going to try and develop some good habits. While they sometimes feel like a chore to adhere to, if you continue on in data science, good git practice will save you some day.\nProblem Sets 1. Make a New Repository All git repositories start with an initialization. For us, that will usually mean starting a new R studio project. It is possible to initialize a git repo in a project that you already have files in, and we will cover that process later in the semester. For now, in the upper right corner of R studio, create a project in a new directory called git_worksheet, and make sure the \u0026ldquo;Create git Repository\u0026rdquo; box is checked when you do.\nOnce the project is created, find the folder in your file browser or finder window, and double click on the \u0026ldquo;git_worksheet.Rproj\u0026rdquo; file to open that R project. In the upper right pane, you should see that there is a tab that says \u0026ldquo;git.\u0026rdquo;\n2. Your First Commit Now that we have a project with a git repository, we can start adding files that we want to keep track of. In general, you want to commit all code files, but not data files. Data files are much larger than what git was made for, and you will quickly run out of storage space on sites like Github if you push them. It is also critically important that you do not commit and files with sensitive information like passwords. Other people will be able to see them, as git is in no way encrypted. It is also very difficult to remove a file from git\u0026rsquo;s memory; it was made to save things!\nNote\nNever commit sensitive files like passwords of API keys.\nCreate a new R script file in your project directory called data_load. Inside this script, in the fist line write a comment (using #) that says \u0026ldquo;This script downloads the data\u0026rdquo;. Next copy the following code into the script.\nsurvey = read.csv(\u0026quot;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026quot;) write.csv(survey, \u0026quot;./survey_data.csv\u0026quot;, row.names = FALSE) This code will read our class survey data, and the write or save the data in our project directory. Execute both the read and write functions, then save that file once you are done and close it.\nLook at the git panel in the upper right pane of R Studio. You should now see (at least) two files there, our survey_code.R script, and the survey_data.csv file we just saved. Both should have yellow question marks next to them. That means git is not currently keeping track of those files.\nWe want to tell git to start tracking our survey_code.R script. To do so, click on the white check box under the \u0026ldquo;Staged\u0026rdquo; column next to survey_code.R. Once you click that, the file is staged, meaning when we make our next commit to the git timeline, that file will be saved.\nLet\u0026rsquo;s make out first commit. In the git pane, click on the \u0026ldquo;Commit\u0026rdquo; button. This will open the commit window. In this window we will see all our files again on the top left, as well as two new areas. In the top right is a box labeled \u0026ldquo;Commit message.\u0026rdquo; This is where you can write a message that will appear on the git timeline describing what you are adding or changing in this commit. For now, type in \u0026ldquo;adding the survey_code script.\u0026rdquo; Press the \u0026ldquo;Commit\u0026rdquo; button once you are done. A progress window will pop up letting you know what is happening. Once it stops changing, you can close it and the commit window.\n3. Making a Change We\u0026rsquo;ve now saved a checkpoint of our survey_code.R script, that we can return to at any point in the future. We can even delete it and bring it back from oblivion! For now, we\u0026rsquo;ll just make some changes to it.\nLet\u0026rsquo;s add some lines of analysis to the script. open it up, and between our read and write scripts, add in the following and save your file:\n# Check the varaibles str(survey) # get the mean hours of sleep mean(survey$hours_sleep) Warning\ngit can only ever see changes to your file after you have saved the file.\nNow that we\u0026rsquo;ve saved out file, it should appear in our git panel again with a blue \u0026ldquo;M\u0026rdquo; next to it, signifying the file has been \u0026ldquo;Modified.\u0026rdquo; Repeat the process of staging it (clicking check box), and committing it (going into the commit menu, adding a message, and pressing commit). We have now added another checkpoint to out git timeline.\n4. Adding More Files Thus far, working with one file seems a lot like saving with extra steps. The true value of git starts to appear once we have multiple files in a project. Create a new R script file called octocat_load.R, and fill it with the following:\n# Code to load in Octocat (github mascot) art octocat = readLines(\u0026quot;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/6_git/octocat.txt\u0026quot;) writeLines(octocat, \u0026quot;./octocat.txt\u0026quot;) readLines works like read.csv in that it is a function to load data into R. Instead of loading CSVs though, it loads text files. Execute this code and then commit the octocat_load.R, but not the newly created octocat.txt file.\nCreate another new R script called octocat_print.R. Inside this file, copy the following code into it:\n# to load the local octocat data and print it octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) print(octocat) Save this file and commit it.\nCreate one last script called octocat_count.R. In this script, copy the following:\n# to count the lines in octocat.txt octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) length(octocat) Save this file and commit it. We now have a toy example of a fairly common data science workflow; get the data, inspect the data, and perform analyses on the data.\n5. Breaking Things Now that we have out mini data science workflow, we can start to modify it. Start by opening octocat_load.R. We can replace the readLines function to load the local version of octocat, because we no longer need to grab it from the internet. Replace:\noctocat = readLines(\u0026quot;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/6_git/octocat.txt\u0026quot;) With\noctocat = readLines(\u0026quot;./octocat.txt\u0026quot;) Save the file and commit the changes.\nSay we want to quickly modify our octocat art by adding an extra line for a caption. Create a new script called octocat_modify.R and add the following code to it:\n# to add a caption to octocat octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) octocat = c(octocat, \u0026quot;ASCII Art of the Octocat Mascot for Github\u0026quot;) writeChar(octocat, \u0026quot;./octocat.txt\u0026quot;) We use c() here to combine octocat with our caption, and then assign it back to our octocat object. Save the file, execute it, and commit it.\nGreat, now we have out data updated, let\u0026rsquo;s open up our octocat_print.R and run it again to see our beautiful art again.\nUh-oh. It doesn\u0026rsquo;t work anymore. You may have noticed we used the wrong function to save out modified octocat object (we used writeChar rather than writeLines). That\u0026rsquo;s fine, we can load the data in again in our octocat_load.R script\u0026hellip; but we can\u0026rsquo;t because we changed that script to use the local copy which we just broke.\nTime to power up the time machine.\n6. Time Travel for Beginners In the git pane, click on the \u0026ldquo;History\u0026rdquo; button to open up the git timeline. The history window is broken in to to main parts. At the top you have your git timeline, which shows all of your commits in this project. The timeline shows you commit messages, the author of those commits, when the commit happened, and an \u0026ldquo;SHA\u0026rdquo; which you can think of as a unique ID for that commit. On the bottom is the diff or \u0026ldquo;difference\u0026rdquo; window. It will show you what files were changed in that commit, and how they changes. Sections in green were added, while sections in red were removed.\nFind the commit in the timeline where we changed octocat_load.R. Inside the diff window, on the box labeled octocat_load.R, click on the \u0026ldquo;View file @ ########\u0026rdquo; button in the upper right of the box. This will open the file as it was when you committed it. Use this to fix our octocat_load.R script, and save an working copy of octocat.txt again.\n7. To the Cloud Now we\u0026rsquo;re going to go over how to set our new repo up on Github. Head to https://github.com/ and in the left hand side bar, click on the green \u0026ldquo;New\u0026rdquo; button. This will take us to the screen to create a new repo. Enter a name under \u0026ldquo;Repository name\u0026rdquo; and then scroll to the bottom of the page and click \u0026ldquo;Create repository.\u0026rdquo;\nYou will now see a page called \u0026ldquo;Quick Setup.\u0026rdquo; Look at the second box that says \u0026ldquo;\u0026hellip;or push an existing repository from the command line.\u0026rdquo; We are going to use these commands to link our local repo with the one on Github. In R Studio, look at the lower left console pane, and click on the \u0026ldquo;Terminal\u0026rdquo; tab. Enter the three lines of code from Github into the terminal one by one. They should look like this (but use the ones from Github, not these!):\ngit remote add origin git@github.com:\u0026lt;REPO-DETAILS\u0026gt; git branch -M main git push -u origin main Once you have done that, right click or command click anywhere on R Studio and select \u0026ldquo;Reload.\u0026rdquo; The screen will go blank a moment and return. When it does, go look at the git pane in the upper right. You will notice you now have the option to click on the \u0026ldquo;Pull\u0026rdquo; and \u0026ldquo;Push\u0026rdquo; buttons. Click \u0026ldquo;Push\u0026rdquo; now, wait for the process to finish, then refresh the page for your new repo on github. You should see your files there!\n8. Conflict So far so good, but sometimes things go awry. On Github, click on the octocat_count.R script to be taken to its page. In the toolbar above the code, on the right hand side, you will see a pencil icon. Click that to edit the file. Add a new comment to the second line that says:\n# Conflict! Scroll to the bottom of the page and click \u0026ldquo;Commit Changes.\u0026rdquo; Now, in R Studio, open up octocat_count.R and on line 2 there, add a comment that says:\n# It happens. Save the file and commit it using the git pane.\nNow, press the \u0026ldquo;Pull\u0026rdquo; button in the git pane. This time an error will pop up saying you have a conflict. A conflict happens whenever git can\u0026rsquo;t reconcile the differences between two versions of the same file, so it will ask you to resolve the conflict. In R Studio, around line 2 in octocat_count.R, you should now see the following:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD # it happens ======= # Conflict! \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; \u0026lt;NUMBERS AND LETTERS\u0026gt; This is git pointing out where the two versions of the file are different. All the differences will exist between the two rows of arrows, the \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; and \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;. The line of equal signs, =======, separates the two versions.\nTo resolve this, we need to pick which version we want to keep. For now, edit this area, so only the comments that says # Conflict! remains. That means you should also delete the \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD, equal signs, and \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; \u0026lt;NUMBERS AND LETTERS\u0026gt;. Save the file and commit it again. After you commit the changes, press the \u0026ldquo;Push\u0026rdquo; button to update github.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/r_packages/","title":"R Packages","tags":[],"description":"","content":" Overview Overview R uses a number of packages to work with data, which are largely community created. This means many of them do not come pre-installed with R. Here is a list of packages we will use this semester. You should be able to paste this into the R console and press enter to install them all at once.\ninstall.packages(\u0026quot;tidyverse\u0026quot;); install.packages(\u0026quot;dplyr\u0026quot;); install.packages(\u0026quot;skimr\u0026quot;); install.packages(\u0026quot;ggplot2\u0026quot;); install.packages(\u0026quot;mosaic\u0026quot;); install.packages(\u0026quot;plotly\u0026quot;); install.packages(\u0026quot;todor\u0026quot;); install.packages(\u0026quot;compareDF\u0026quot;); install.packages(\u0026quot;future\u0026quot;); install.packages(\u0026quot;rvest\u0026quot;); install.packages(\u0026quot;rrefine\u0026quot;); install.packages(\u0026quot;tidycensus\u0026quot;); install.packages(\u0026quot;RSQLite\u0026quot;); install.packages(\u0026quot;sf\u0026quot;); install.packages(\u0026quot;statnet\u0026quot;) To make sure everything was installed, run the following command. It should return TRUE.\nall(c(\u0026quot;tidyverse\u0026quot;, \u0026quot;dplyr\u0026quot;, \u0026quot;skimr\u0026quot;, \u0026quot;ggplot2\u0026quot;, \u0026quot;mosaic\u0026quot;, \u0026quot;plotly\u0026quot;, \u0026quot;todor\u0026quot;, \u0026quot;compareDF\u0026quot;, \u0026quot;future\u0026quot;, \u0026quot;rvest\u0026quot;, \u0026quot;rrefine\u0026quot;, \u0026quot;tidycensus\u0026quot;, \u0026quot;RSQLite\u0026quot;, \u0026quot;sf\u0026quot;, \u0026quot;statnet\u0026quot;) %in% installed.packages()[,\u0026quot;Package\u0026quot;]) "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/slides/","title":"Slides","tags":[],"description":"","content":" Lecture Slides 01_1_intro.html (12 MB) 01_2_what_is_data.html (21 MB) 02_2_intro_r.html (8 MB) 03_1_git.html (4 MB) 03_2_eda.html (3 MB) 04_02_agg_merge.html (6 MB) 04_1_tidy.html (5 MB) 05_1_adv_plot.html (23 MB) 05_2_dyn_plot.html (26 MB) 06_1_ethics.html (5 MB) 06_2_project_1.html (4 MB) 07_1_functions.html (3 MB) 07_2_conditions_debugging.html (6 MB) 08_1_iteration.html (3 MB) 08_2_list_apply.html (5 MB) "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/install_guide/wsl/","title":"[WIN ONLY] Windows Subsystem for Linux","tags":[],"description":"","content":" Overview Check your windows version Windows version larger than (or equal to) 19041 Windows version smaller than 19041 Verifying your install Overview Windows now has the ability to install a linux operating system on your machine without the use of an emulator. This gives you a full-featured linux environment that can interact with your normal files.\nCheck your windows version First, please check the build version of Windows that you are using. This can be done by selecting the Windows logo key + r on your keyboard. Once you do that, you should see the windows Run prompt:\nType the command \u0026ldquo;winver\u0026rdquo; (no quotes) into the prompt, as depicted in the image above, and hit enter. You should see a screen similar to this:\nYour build version number is the one that follows the \u0026ldquo;Windows Build\u0026rdquo; text (as highlighted in the above image). Depending on whether your build number is larger or smaller than 19041, please follow the appropriate directions below:\nLarger than (or equal to) 19041 [Smaller than 19041][Windows version larger smaller than 19041] Windows version larger than (or equal to) 19041 In the start menu, search for \u0026ldquo;Powershell\u0026rdquo;, right click and select \u0026ldquo;run as administrator\u0026rdquo;.\nA small blue window should open with a flashing cursor. Please type wsl --install and hit enter:\nThis will take a few minutes to install everything. Once it is finished, please skip to verifying your install.\nIf the above installation did not work- please try to install [using these instructions][Windows version larger smaller than 19041] (even if your version is not smaller than 19041).\nWindows version smaller than 19041 In the start menu, search for \u0026ldquo;Turn Windows features on or off\u0026rdquo; and open that settings window.\nIn the settings window, scroll down to \u0026ldquo;Windows Subsystem for Linux\u0026rdquo;, check the box next to it, and select OK at the bottom of the window.\nYou will not need to restart your computer. Once you have rebooted, open the Windows Store from the start menu.\nIn the Windows Store, search for Ubuntu, and select the version-less one unless you have a reason to pick a specific version.\nOn the Ubuntu page, select Get to install.\nVerifying your install In the start menu, search for and run Ubuntu.\nIf a terminal window opens, you should be good to go!\nThis creates an entirely new operating system on your machine. Thus, things like your git configuration and SSH key for Github will not carry over! You will need to configure git again, and create a new SSH key for this operating system.\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/7_basic_plots/","title":"Exploratory data analyses Worksheet","tags":[],"description":"","content":" Overview Problem Sets 1. Load the Data 2. Numeric Summaries 3. Tables 4. Plots! Plots! Plots! Overview Exploratory data analyses (EDA) are one of the first steps in any data project. The goal is literally to explore the data; the better you know your data, the better you can work with it. What are the dimensions (rows and columns)? What types of data are there? Does anything need to be re-coded? What is the missingness like? These are the sort of questions we try to figure out during EDA.\nToday we will be using the basic plots built into R. They are all pretty ugly, and you probably won\u0026rsquo;t want to share them anywhere. Don\u0026rsquo;t worry though, we\u0026rsquo;ll have two full days later on dedicated to making great looking plots. The simplicity of the basic R functions limits the number of new concepts to learn at once.\nToday we will be looking some biological data, specifically penguins ð§. The palmerpenguins R package grants easy access to data from Palmer Station in Antarctica, part of the Long-Term Ecological Research program. It contains data on 344 penguins, and aside from being fun, is a great way to learn about data visualization.\nThe Palmer Archipelago penguins. Artwork by @allison_horst. Problem Sets 1. Load the Data First up, we will need to install the palmerpenguins package to get the data, load the package into R, and then put the data into our environment. You can do that with the following commands:\ninstall.packages(\u0026quot;palmerpenguins\u0026quot;) # To install the data library(palmerpenguins) # To load the R package penguins = data.frame(penguins) # To get the data into our environment You should now see the penguins object in your environment page. Go ahead and click on it to see it in the viewer and look around.\n2. Numeric Summaries We\u0026rsquo;ll start our exploration by using the str() function on our penguins dataframe. This will gives us the number of cases (rows) and variables (columns) in our dataframe, and tell us the types of our variables.\nTo get some more details, we will use the skimr package. We installed it during install day, so all we need to so is load it using:\nlibrary(skimr) After that, we are set to skim() our data. Try running skim(penguins).\nThe skim() function gives us a bit more information than str(), and even some miniature visualizations! Let\u0026rsquo;s look through the output and digest some of it. At the top is a \u0026ldquo;Data Summary\u0026rdquo; which provides much the same information as str(), we have the number of rows and columns, plus the number of columns with specific data types.\nThe bottom portion of the skim() output provides information on the variables, separated by their type. First you will see factor variables, which we have discussed but never encountered. Factors are meant for categorical data, with the option of those categories being ordered. A plain old category is essentially and way to describe something, in the case of our penguins, an example is the island they were found on. You could have an ordered factor if there was a clear hierarchy of categories, for example months could be categories, with a clear order of January first to December last.\nThe numeric variables are also shown, along with some important numbers such as their mean and standard deviation. Something shared between both variables types is the n_missing, or \u0026ldquo;number missing.\u0026rdquo; This lets us know how many NAs are in the variable column.\nHow many missing values are there in the bill_length_mm variable?\n2 NAs\nIf we wanted to get some information about one particular column in our data, we could also use the summary() function. summary() is a very versatile function in R, whose behavior changes depending on what you give it as an argument. For now, let\u0026rsquo;s try it on a numeric vector like penguins$bill_depth_mm using summary(penguins$bill_depth_mm). The result is called a \u0026ldquo;five-number summary\u0026rdquo; (even though there are six numbers here).\nThe five-number summary includes the following (and the mean, because it\u0026rsquo;s useful): * The minimum value in the vector * The first quartile (25% of the data falls below this value) * The mean and median (50% of the data falls below this value) * The 3rd quartile (75% of the data falls below this value) * and the max.\nThese five numbers give us a sense of how what the spread of the data is, and can clue us in to potential outliers later. It helpfully also tells us how many values are NAs!\nUse summary() on the flipper_length_mm column. What is the 1st quartile?\nsummary(penguins$flipper_length_mm)\n3. Tables The table() function is a super simple yet effective way of understanding your data set. It simply counts how many of each category exists in your data, and reports that back to you. For example, if you run:\ntable(penguins$island) It will tell you how many of our cases (rows) appeared on each island in the data set. You can also pass along two different categories to find out how many cases fir in to two categories at once. For example try:\ntable(penguins$island, penguins$species) What is the resulting table telling us? Explain what the 0s in this table mean.\nThis table shows how many penguins of each species were seen on each island. The 0s mean there were no penguins of that type seen on those islands.\n4. Plots! Plots! Plots! Bar Plot A bar plot is most often used to show the frequency of categorical data, or how many times do things of this category show up in the data set? In this case, we will be plotting the frequency of each category of penguin in our dataset. We have three species of penguins in our data, the Adelle, Chinstrap, and Gentoo penguins. Try using the following:\nbarplot(table(penguins$species)) Here our barplot shows the species of penguins on the X (horizontal) axis, and the frequency on the Y (vertical) axis. So from a glance, you can see there are about 130-ish Gentoo penguins in the data. You\u0026rsquo;ll notice that we actually used two functions here, one inside the other. This is the same as doing:\npengi_table = table(penguins$species) barplot(pengi_table) Both are valid options, but it\u0026rsquo;s usually not a good idea to layer too many functions inside one another.\nThis question may be tricky. Why do we need to give barplot() the output from table(penguins$species), rather than just penguins$species? Check the help file for barplot() using ?.\nThis table shows how many penguins of each species were seen on each island. The 0s mean there were no penguins of that type seen on those islands.\nHistogram Histograms are like bar plots, but for variables that are continuous, meaning there are no boundaries between them; this pretty much always means numeric data. Where barplots look at distinct categories, histograms look at numbers. Make a histogram of the body_mass_g variable using hist(penguins$body_mass_g).\nHere we see the body mass of our penguins on the X axis, and the frequency of how many penguins have about that body mass on the Y axis. There are a few more choices to be made with a histogram, specifically how many bins we want, or how our variable on the X axis will be grouped. We can control this using the breaks argument to hist(). The best way to illustrate this is to try it:\nRun the following three commands. What is the difference in the plots they produce?\nhist(penguins$body_mass_g, breaks = 10) hist(penguins$body_mass_g, breaks = 50) hist(penguins$body_mass_g, breaks = 5)\nThe number of breaks determines the number of bins. More bins seperate the body mass into smaller categories for plotting.\nScatter Plot A scatter plot is used to look at two continuous variables and compare them to each other, often to look for relationships. One relationship that may make sense in between body_mass_g and bill_length_mm; in other words, do larger penguins have longer bills? We can find out using the following:\nplot(penguins$bill_length_mm, penguins$body_mass_g) Here we see bill_length_mm on the X axis, and body_mass_g on the Y axis. Each dot in the plot is once case in our data. Unlike other plots, the Y axis here is not frequency, but instead another continuous variable.\nMake a scatter plot looking at the relationship between body_mass_g and flipper_length_mm.\nplot(penguins$flipper_length_mm, penguins$body_mass_g)\nLine Plot Line plots often look at change over time in a variable. You\u0026rsquo;ve most likely seen these before, especially on the news. Line plots are like scatter plots, but the points are connected. Unlike the other plots, we need to do some pre-processing to make a line plot.\nLet\u0026rsquo;s say we want to look at the mean body mass of all penguins over time. We want years on the X axis, and mean body mass of penguins on the Y. To do this, let\u0026rsquo;s make a new data frame containing a column for year, and a column for mean body mass:\nline_data = data.frame(\u0026quot;year\u0026quot; = c(2007, 2008, 2009), \u0026quot;mean_mass\u0026quot; = c( mean(penguins[penguins$year == 2007, \u0026quot;body_mass_g\u0026quot;], na.rm = TRUE), mean(penguins[penguins$year == 2008, \u0026quot;body_mass_g\u0026quot;], na.rm = TRUE), mean(penguins[penguins$year == 2009, \u0026quot;body_mass_g\u0026quot;], na.rm = TRUE) )) We can then use our new data frame to make a line plot (giving the type = \u0026quot;l\u0026quot; argument to say we want a line):\nplot(line_data, type = \u0026quot;l\u0026quot;) Seems 2008 was a good year for penguins! Plots like this are important for monitoring the health of an animal population over time. If there was a steady downward trend in penguin mass, we may be concerned they aren\u0026rsquo;t getting all the food they need. Later on we will make more complex plots where we can see the trends for each species of penguin.\nBox Plot The last of the big five plot types is the box plot. The box plot is essentially a visual representation of the five-number summary we get from summary(). You can make a boxplot using the boxplot() function. Let\u0026rsquo;s test it out with both of the following:\nsummary(penguins$bill_depth_mm) boxplot(penguins$bill_depth_mm) Compare the results of summary() and boxplot() above. How do each of the components of a box plot align with the five-number summary (the mean isn\u0026rsquo;t plotted).\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/9_tidy/","title":"Tidy Data Worksheet","tags":[],"description":"","content":" Overview Problem Sets 1. What is \u0026ldquo;Tidy\u0026rdquo; Data 2. Re-Coding Data Types 3. Re-Coding Specific Data 4. Pivoting Data Overview Making data tidy and keeping data tidy is one of the key responsibilities of any data scientist. All of our analyses, and by extension our ability to learn from data, is contingent on the predictable format of each cell of our dataframes containing the one piece of information we want to look at. Data is rarely in the format to start with, and it often takes a significant amount of time to get it that way. The process of getting data into a tidy format is often called data cleaning, data wrangling, or data munging.\nToday we will learn a few early data cleaning skills, which we will add to for the rest of the semester. We will start with re-coding data, or changing how data is represented. We will then move on to changing the shape of data through pivoting.\nProblem Sets 1. What is \u0026ldquo;Tidy\u0026rdquo; Data Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59, 1\u0026ndash;23. https://doi.org/10.18637/jss.v059.i10\nAll \u0026ldquo;tidy\u0026rdquo; data follows a predictable format. This format allows for pre-built code to work on this data in a predictable way, making the data science workflow significantly easier and less prone to errors. A \u0026ldquo;tidy\u0026rdquo; dataset adheres to the following format: all observations are rows, all variables are columns, and each cell is one value representing that intersection.\nFrom R for Data Science If seen in a cell from a dataframe, are the following examples of \u0026ldquo;tidy\u0026rdquo; data? Respond with TRUE or FALSE, and if FALSE explain why.\n42 \u0026ldquo;3 cats\u0026rdquo; \u0026ldquo;Orange\u0026rdquo; \u0026ldquo;1993-2010\u0026rdquo; \u0026ldquo;NAME: Samantha Carter\u0026rdquo; TRUE FALSE. It is mixing two distinct pieces of information: how many things there were and what those things were. TRUE FALSE. It contains two pieces of data, a start year and an end year. If this was the difference between those two years, it would be tidy. FALSE. It contains a definition of the data, and the data itself. If we want this data to be about names, it should be in a NAME column. 2. Re-Coding Data Types When we re-code data, we don\u0026rsquo;t actually change the data; for example, we may re-code a variable that is initially 0 and 1 to TRUE and FALSE. Both of these represent the same thing, but R is better able to work on this logical data type if it is actually a logical in R, rather than a numeric representation.\nWe also often see the same problem arise with character data. Given character is the most permissive data type in R \u0026ndash; meaning it can store nearly anything \u0026ndash; sometimes R will use it to be safe, even if the data is not meant to be a character. Take the following vector as an example:\n[1] \u0026quot;1\u0026quot; \u0026quot;5\u0026quot; \u0026quot;8\u0026quot; \u0026quot;3\u0026quot; \u0026quot;1\u0026quot; \u0026quot;5\u0026quot; It looks like a series of numbers, so it must be numeric, right? Unfortunately not. We can tell from the quotes around the numbers that R is actually treating this vector of numbers as a character vector. This will cause problems if we try to do any analysis with this vector, like say getting the mean:\nmean(char_nums) Warning in mean.default(char_nums): argument is not numeric or logical: returning NA [1] NA We get an error saying the argument to our mean() function is not numeric or logical, even though we can clearly tell the vector is all numbers! This is another case where re-coding comes in. We want to make sure R is understanding the data the same way we are. To change how this data is represented in R, we can coerce the class of the data to something else using the as.XXXX() family of functions.\nThere is an as.XXXX() function for every data type:\nas.character() as.factor() as.numeric() as.logical() Try re-coding the following two vectors using each of the as.XXXX() functions above. Can you understand each of the results? Some will fail!\ntest_vec_1 \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;45\u0026quot;, \u0026quot;22\u0026quot;, \u0026quot;99\u0026quot;, \u0026quot;45\u0026quot;) test_vec_2 \u0026lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) tect_vec_1 as.character() - Nothing happens. It\u0026rsquo;s already a character. as.factor() - Turns the characters into a factor as.numeric() - Turns our numbers into a proper numeric vector as.logical() - R dosn\u0026rsquo;t know how to turn these characters into logical test_vec_2 as.character() - Changes our TRUEs and FALSEs into the WORD \u0026lsquo;TRUE\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo; as.factor() - Turns the logicals into a factor, they no longer mean TRUE and FALSE, just categories with those names as.numeric() - TRUE and FALSE are often coded as 1 and 0, so R knows how to make that change as.logical() - Nothing happens. 3. Re-Coding Specific Data We can flex our sub-setting muscles and use them to recode specific variables. Re-coding in this way is really just replacing the values, so you need to be very careful when doing so. The key insight here is that whenever you sub-set data, you can opt to modify that data as well.\nRun the following code to load in our survey data to practice on:\nsurvey \u0026lt;- read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) Let\u0026rsquo;s start with changing a single value. You won\u0026rsquo;t actually do this often, but it\u0026rsquo;s good to know. We can replace single cell by giving R the row and column of the specific value we want. For example, go look at the survey dataframe, and scroll through the fav_art column. There are some NAs in there. If we wanted to change one of those NAs because we later learned an individual had no favorite art, we could use our sub-setting to do that. Try the following:\nsurvey[2, 13] [1] NA survey[2, 13] = \u0026#34;None\u0026#34; Now go look at our survey dataframe again. We can see that we replaced the value in the 2nd row, and the 13th column, or the fav_art of the person whose favorite character is \u0026ldquo;Doreamon\u0026rdquo;. We can use any of our sub-setting tricks in the same way, such as calling columns by name. For example: survey[2, 13] = \u0026quot;None\u0026quot; and survey[2, \u0026quot;fav_art\u0026quot;] = \u0026quot;None\u0026quot; will do the exact same thing.\nFor a more practical example, let\u0026rsquo;s look at our nerd column. This question asked if people identified with the label \u0026ldquo;nerd\u0026rdquo; to describe themselves. Right now it is a logical vector of TRUEs and FALSEs. Say we wanted to change that to \u0026ldquo;Yes\u0026rdquo; and \u0026ldquo;No\u0026rdquo; for the sake of presentation. We can use our sub-setting knowledge to do so:\nsurvey[survey$nerd == TRUE, \u0026#34;nerd\u0026#34;] \u0026lt;- \u0026#34;Yes\u0026#34; survey[survey$nerd == FALSE, \u0026#34;nerd\u0026#34;] \u0026lt;- \u0026#34;No\u0026#34; Check out our nerd column now. The above code asked R to: Give me the survey dataframe, such that the rows are when nerd was TRUE, and the column was nerd, and assign all of those values to \u0026ldquo;Yes.\u0026rdquo;\nOn the other hand, the next command asked R to: Give me the survey dataframe, such that the rows are when nerd was FALSE, and the column was nerd, and assign all of those values to \u0026ldquo;No.\u0026rdquo;\nNow, this is somewhat dangerous because we are replacing our data using this method. let\u0026rsquo;s try a less destructive technique on another column. We can accomplish much the same thing using the ifelse() function. ifelse() takes three main arguments: a test of some kind, what to do if that test is TRUE, and what to do if that test is FALSE. For example, run the following lines:\nFirst, we looked at the data, and what it actually is.\nsurvey$fict [1] \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Non-fiction\u0026quot; \u0026quot;Non-fiction\u0026quot; \u0026quot;Fiction\u0026quot; [6] \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; [11] \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; \u0026quot;Fiction\u0026quot; Next, we test if that data matched a certain condition using our == comparison, meaning \u0026ldquo;equal to\u0026rdquo;.\nsurvey$fict == \u0026#34;Fiction\u0026#34; [1] TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [13] TRUE TRUE TRUE Last, we use ifelse() to use that test to give us outputs conditional on if the results was TRUE or FALSE.\nifelse(survey$fict == \u0026#34;Fiction\u0026#34;, \u0026#34;I like fiction more\u0026#34;, \u0026#34;I like non-fiction more\u0026#34;) [1] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [3] \u0026quot;I like non-fiction more\u0026quot; \u0026quot;I like non-fiction more\u0026quot; [5] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [7] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [9] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [11] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [13] \u0026quot;I like fiction more\u0026quot; \u0026quot;I like fiction more\u0026quot; [15] \u0026quot;I like fiction more\u0026quot; We can take that result and add it to our dataframe like so:\nsurvey$fict_phrase = ifelse(survey$fict == \u0026#34;Fiction\u0026#34;, \u0026#34;I like fiction more\u0026#34;, \u0026#34;I like non-fiction more\u0026#34;) Now we\u0026rsquo;ve re-coded some data, but still have our original!\nCreate a new column in our survey dataframe called hotdog_text, re-coding the hotdog column to: \u0026ldquo;I think a hot dog is a sandwich\u0026rdquo; for TRUE and \u0026ldquo;I do not think a hot dog is a sandwich\u0026rdquo; for FALSE.\nsurvey$hotdog_text = ifelse(survey$fict == TRUE, \u0026lsquo;I think a hot dog is a sandwich\u0026rsquo;, \u0026lsquo;I do not think a hot dog is a sandwich\u0026rsquo;)\n4. Pivoting Data Pivoting data takes some time to wrap your head around. The following graphic is very helpful in that process:\nData Science Workshops Here we see two representations of the same data. The \u0026ldquo;wide\u0026rdquo; data is what we have been working with so far. Each row is a case or observation, and every column is a variable for that case. We can see cases 1, 2, and 3 in the figure, with variables A1, A2, and A3.\nThe \u0026ldquo;Long\u0026rdquo; format changes this slightly. Instead of having a column for each variable, there are instead two columns which represent all possible variables, a \u0026ldquo;key\u0026rdquo; column, and a \u0026ldquo;value\u0026rdquo; column. Each of our previous column names are now stored in the \u0026ldquo;Key\u0026rdquo; column, and the value from their cells is stored in the \u0026ldquo;Val.\u0026rdquo; column. It\u0026rsquo;s best to look at an example ourselves. Run the following:\nlibrary(tidyr) survey_long \u0026lt;- survey |\u0026gt; pivot_longer(cols = -fav_char, values_transform = as.character) survey_wide \u0026lt;- survey_long |\u0026gt; pivot_wider() We now have two versions of our survey dataframe; one \u0026ldquo;long\u0026rdquo; and one \u0026ldquo;wide.\u0026rdquo; Open up the \u0026ldquo;wide\u0026rdquo; dataframe. It should look just like our normal survey dataframe. Now open the \u0026ldquo;long\u0026rdquo; version, and look through the \u0026ldquo;name\u0026rdquo; column. You should recognize all of our former variable columns, the values of the related cells stored in the new \u0026ldquo;value\u0026rdquo; column, and fav_char being repeated multiple times for each column.\nThe code we used to do this comes from the tidyr package, part of the tidyverse family of packages. Because of this, it behaves a little differently from all other R code. Namely, we use a pipe |\u0026gt; to \u0026ldquo;pass\u0026rdquo; the data to the pivot_X() function, rather than include it as an argument inside the function itself. We also use a format for our arguments that no other functions use, the \u0026ldquo;-fav_char\u0026rdquo; argument. In tidy language, we are saying we want all the columns except our fav_chars to be turned from wide to long; fav_chars will be used as our ID to identify cases. While handy, I need to stress that this syntax will not work with other kinds of functions, only tidyverse family functions.\nWhy would we ever want to pivot our data like this? Sometimes it is helpful for certain visualizations and analyses, but it is also more efficient from a computer\u0026rsquo;s perspective. In this format, if we had information for a variable, but we did not have that information for everyone, instead of including a whole column with a lot of NAs, we could simply omit that information from this table and nothing would really change.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/10_agg_merge/","title":"Aggregation and Merging Worksheet","tags":[],"description":"","content":" Overview Problem Sets 1. Aggregation using group_by() and summarise() 2. Binding Dataframes 3. Merging Dataframes Overview Aside from sub-setting, aggregating and merging data many be the most common task of a data scientist.\nProblem Sets 1. Aggregation using group_by() and summarise() Often times we want to know how different categories in our data compare to each other. This comparison let\u0026rsquo;s us get a sense if one group is in some way different than others, and potentially make decisions based on that difference. For a simple example, lets look at our class survey data.\nsurvey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) If we skim() our data, we have quite a few variables we could group our cases by to make comparisons.\nlibrary(skimr) skim(survey) Name survey Number of rows 15 Number of columns 23 _______________________ Column type frequency: character 14 logical 6 numeric 3 ________________________ Group variables None Data summary\nVariable type: character\nskim_variable n_missing complete_rate min max empty n_unique whitespace fav_char 0 1.00 4 39 0 15 0 fav_color 1 0.93 3 50 0 10 0 b_month 0 1.00 3 9 0 9 0 pets 0 1.00 3 17 0 8 0 fav_art 3 0.80 17 1158 0 12 0 coffee_days 7 0.53 6 62 0 7 0 tea_days 10 0.33 6 17 0 5 0 soda.pop_days 9 0.40 7 62 0 5 0 juice_days 4 0.73 6 62 0 8 0 none_days 10 0.33 17 45 0 5 0 lt_location 3 0.80 7 47 0 12 0 fict 0 1.00 7 11 0 2 0 recreation 0 1.00 51 232 0 15 0 key 0 1.00 6 41 0 15 0 Variable type: logical\nskim_variable n_missing complete_rate mean count major 15 0 NaN : other_classes 15 0 NaN : car 0 1 0.33 FAL: 10, TRU: 5 pineapple_pizza 0 1 0.67 TRU: 10, FAL: 5 nerd 0 1 0.73 TRU: 11, FAL: 4 hotdog 0 1 0.47 FAL: 8, TRU: 7 Variable type: numeric\nskim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist fav_num 0 1 15.60 14.92 3 7 9 20.5 56 âââââ mint_choc 0 1 3.60 1.72 1 2 4 5.0 5 âââââ hours_sleep 0 1 6.87 0.74 6 6 7 7.0 8 âââââ For now, let us explore if there is any difference between those that do and do not categorize a hot dog as a sandwich, given it nearly evenly splits the class. We ca do this using the group_by() function. In essence, we want to group our data by TRUE and FALSE in our hotdog column, and then perform some other analysis. We can direct that analysis using the summarise() function. For example:\nlibrary(dplyr) Attaching package: 'dplyr' The following objects are masked from 'package:stats': filter, lag The following objects are masked from 'package:base': intersect, setdiff, setequal, union survey |\u0026gt; group_by(hotdog) |\u0026gt; summarise(sleep = mean(hours_sleep)) # A tibble: 2 Ã 2 hotdog sleep \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; 1 FALSE 7.12 2 TRUE 6.57 In this example, we take our survey data, pipe it to group_by() where we specific we want to group by hotdog status, then pipe that grouped data to summarize() where we say we want the mean of our hours_sleep column in an output called sleep. We can see in our example that those that do not consider a hot dog a sandwich, at least in our class, manage to get slightly more sleep on average.\nTry copying the format from our example, but group by birthday month, and get both the mean of hours_sleep and a table of nerd. Do you see any concerns with the output?\nsurvey |\u0026gt; group_by(b_month) |\u0026gt; summarise( sleep = mean(hours_sleep), nerd = table(nerd))\nWe have so many groups, that many most likely only have one member anyway, so no real use grouping.\n2. Binding Dataframes To start, let\u0026rsquo;s load in some data to practice our merging. Execute the following to create several dataframes we will use to practice.\nsource(\u0026#34;https://raw.githubusercontent.com/Epsian/table_join_lecture/main/src/2_merge_examples.R\u0026#34;) Join Vertically using rbind If you have two dataframes and want to combine them, you need to decide what direction you want to combine them in. If you would like to combine them vertically, or stack the rows on top of each other, you can use rbind or row bind. rbind is helpful if you have two dataframes with the same columns, and you want to combine the cases (rows). However, it will not work if the columns are different. rbind accepts an arbitrary number of dataframes, here we use two: upper_rbind_df and lower_rbind_df.\nrbind_outcome = rbind(upper_rbind_df, lower_rbind_df) rbind example 1 Try using rbind to combine our new outcome dataframe with upper_rbind_df again. Why does this work?\nBoth of the dataframes still have the same columns. Even if rows are repeated, they can still stack vertically.\nJoin Horizontally using cbind If you would like to combine two dataframes horizontally, so that you add more columns on to a dataframe, you can use cbind or column bind. The cbind function takes an arbitrary number of dataframes as it\u0026rsquo;s arguments.\ncbind_outcome = cbind(left_cbind_df, right_cbind_df) cbind example 1 We can see out new outcome dataframe was created as expected. This method can work, but you must be very confident about the structure of your data.\nTry using cbind to combine left_cbind_df with our rbind_outcome dataframe from above. What happens? Why is the result bad?\ncbind(left_cbind_df, rbind_outcome)\nIt works, but produces a result we might not expect with repeated rows data.\n3. Merging Dataframes cbind may work in some situations, but is very \u0026ldquo;brittle\u0026rdquo; \u0026ndash; or easy to break \u0026ndash; with any changes to the data. A more reliable method of merging makes use of a common key between two sources of data. Think of a key like a luggage tag on a suitcase: it\u0026rsquo;s a small bit of information that clearly links to things, even if they are separated. In terms of data, a key can be anything, like a numerical ID or a string, but they must always be unique. Joins that use the key method will be familiar to anyone who has used SQL in the past, as they follow the same naming convention as in that language.\nInner Join An inner join tries to join two sets of data using a common key, keeping only the data that exists in both of the original data sets. This is commonly illustrated using a venn diagram similar to the one below. Only the area highlighted with green will be included in the output.\nInner Join - Venn Diagram In the context of our data, it might look something like the following if we use hometown as our key. We can use the following code to do an inner merge using dplyr\u0026rsquo;s inner_join() function. inner_join() requires three arguments, x and y, which are the dataframes we would like to merge, and by which is the key we would like to join by. We will be using left_merge_df and right_merge_df, and asking R to use hometown as the key to join by.\nlibrary(dplyr) inner_outcome = inner_join(x = left_merge_df, y = right_merge_df, by = \u0026#34;hometown\u0026#34;) Inner Join - Example Data Everything seems in order. We can see that when using an inner join, cases where both dataframes have a single row with our key will be matched and joined into a single output dataframe. However, if there are rows in either data frame without a match, those rows will be dropped from our data.\nCreate a new dataframe using rbind() called double_left which binds two copies of left_merge_df on top of each other. Then perform an inner join with this new double_left and right_merge_df. What happens? What was the problem here?\ndouble_left = rbind(left_merge_df, left_merge_df) inner_join(x = double_left, y = right_merge_df, by = \u0026lsquo;hometown\u0026rsquo;)\nThe matches are performed multiple times. This essentially doubles our data set, which can cause big problems later.\nOuter Join An Outer Join is the opposite of an inner join. Rather than just looking for those rows which have a key in common, it will join every row, regardless of the keys, inserting blank values where there is no match. You can get a sense of this merge with the venn diagram below, where the green indicates good matches, while the orange indicates partial matches.\nOuter Join - Venn Diagram In the context of our data, an outer join may look something like the following, again using hometown as our key. The code for an outer join using dplyr is full_join().\nfull_outcome = full_join(x = left_merge_df, y = right_merge_df, by = \u0026#34;hometown\u0026#34;) Outer Join - Example Data In this example, we see that an outer join will keep all available cases, but we introduce NA values into the resulting dataframe where no match could be made. This may or may not be an issue depending on what your next steps are.\nLeft Join A left join allows you to pick one of the two dataframes you are joining and prioritize it. It essentially takes all of the cases in the dataframe on the \u0026rsquo;left\u0026rsquo; side, and searches in \u0026lsquo;right\u0026rsquo; dataframe to join what it can. Keeping with the venn diagram representations, it would look like the following:\nLeft Join - Venn Diagram Using our example data, a left join would look like this. The R code is similar to the previous examples, in this case it is left_join(). Recall that in the function call, x is the first dataframe, or the \u0026ldquo;left\u0026rdquo; one.\nleft_outcome = left_join(x = left_merge_df, y = right_merge_df, by = \u0026#34;hometown\u0026#34;) Left Join - Example Data We can see that while all of the data from our \u0026rsquo;left\u0026rsquo; side is preserved, unmatched data on the \u0026lsquo;right\u0026rsquo; is discarded. You can also technically do a right join using right_join(), which will do the same thing with sides reversed, but moving your prioritized data set to the left is more common.\nTry performing a right merge which creates the same outcome as our left merge above. Describe the differences. Why are these differences present?\nright_outcome = right_join(x = left_merge_df, y = right_merge_df, by = \u0026lsquo;hometown\u0026rsquo;)\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/12_adv_plot/","title":"Advanced Plotting","tags":[],"description":"","content":" Overview The Data Problem Sets 1. Install Esquisse 2. Learning the Esquisse UI 3. Your First ggplot 4. ggplot2 Code Something like Overview Visualizing data is very important for understanding that data. All sorts of otherwise unknown patterns can slip by if we don\u0026rsquo;t take the time to look at our data. For example:\nDatasaurus - Alberto Cairo While the Datasaurus is fun, it is also a cautionary tale. Along the top of the plot there are various numerical representations of the data, all of which look completely normal! Such oddities can easily slip past us without proper visualization.\nVisualization also serves another crucial purpose; to communicate our findings with other people. Dealing with and understanding data is an acquired skill, one you\u0026rsquo;re already more familiar with than the vast majority of people. Visualizations often allow you to quickly communicate your findings with people who are not as well acquainted with data.\nToday we will be learning some more advanced plotting tools. While we will often rely on the 5 canonical plots we learned before, a lot can be done to spruce them up. We will primarily be using the ggplot2 package for this task. While it is very powerful, it takes more effort to make even a simple ggplot. That effort is often worth it however if the intent is to share your visualization.\nI highly recommend you open up the ggplot2 cheatsheet while you work.\nThe Data Load in the data for today by running the following:\nnbi_hampshire = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/12_adv_plot/data/nbi_hampshire.csv\u0026#34;) Today we will be using data from the 2022 National Bridge Inventory Dataset. Specifically, we will be looking at vehicle bridges in Hampshire county (where we are). We\u0026rsquo;re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\nSTRUCTURE_NUMBER_008 Unique ID for the bridge COUNTY_CODE_003_L Name of the county where the bridge is located ROUTE_PREFIX_005B_L Route signing prefix for the inventory route MAINTENANCE_021_L The actual name(s) of the agency(s) responsible for the maintenance of the structure YEAR_BUILT_027 The year of construction of the structure ADT_029 The average daily traffic volume for the inventory route based on most recent available data STRUCTURE_KIND_043A_L The kind of material and/or design of the structure STRUCTURAL_EVAL_067 A rating of the structural evaluation of the bridge based on inspections of its main structures, substructures, and/or its load ratings BRIDGE_IMP_COST_094 Estimated costs for bridge improvements Problem Sets 1. Install Esquisse Today we will be using a package called esquisse to help us make our plots. It provides an interactive interface to put together the basic components of our plots. It is not perfect, and cannot be used to create a fully realized visualization, but it can often do the first 50% for you pretty quickly. Install it using the following:\ninstall.packages(\u0026#34;esquisse\u0026#34;) You will need to fully close R Studio and restart it after installing esquisse. This is so the addin functionality can work.\n2. Learning the Esquisse UI Once you have restarted R Studio and re-loaded the data, we can start using using esquisse to build draft plots. Start the UI by going to the Addins menu at the top of R Studio and clicking on the \u0026lsquo;ggplot2\u0026rsquo; builder option under esquisse.\nesquisse will load for a second, and then ask you to select a data source. Pick our nbi_hampshire dataframe and click Import Data.\nAfter you have imported your data, you will be taken to the plot builder. I have created a diagram of the different elements below. Along the top are the variables from your dataframe. You can click and drag the variables into the various element areas below. On the left of this section will be a display of what plot esquisse thinks may work well given your variables and elements, but you can click on it to manually change it. In the center is the plot preview, which will automatically update as you make changes. At the bottom is the options bar, with several other menus you can go through to adjust your plot. Importantly, in the far right option menu you can copy the code which will generate the plot you are looking at! You will want to copy that code into your document so you can remake this plot later, and further adjust it.\n3. Your First ggplot Open up the plot builder, and import the nbi_hampshire dataframe. For our first variable, click and drag YEAR_BUILT_027 into the \u0026ldquo;X\u0026rdquo; elements box. This will put YEAR_BUILT_027 on our X axis. Next, click and drag STRUCTURAL_EVAL_067 into the \u0026ldquo;Y\u0026rdquo; element box to place in on the Y axis. In a moment the plot preview area should update, creating a scatter plot.\nYou can continue adding variables to elements to further develop the plot.\nClick and drag the ROUTE_PREFIX_005B_L variable into the \u0026ldquo;color\u0026rdquo; element. How does this change the plot?\nAdds route type as color to the dots in the scatterplot.\nOnce you have added the new element, click on the plot type window next to the variables. In the menu that pops up, select \u0026ldquo;Jitter.\u0026rdquo; A jitter-ed scatter plot adds a little random noise to the dot locations so that multiple dots that are in the same spot can be seen.\nOnce that is done, look toward the bottom of the plot builder at the options menus. In the \u0026ldquo;Labels \u0026amp; Title\u0026rdquo; menu, add a title, labels for the X and Y axes, and a label for the colors.\nYou can add further refinements if you would like. Once you are done, go to the last option menu that says \u0026ldquo;Code.\u0026rdquo; Open that menu to see the ggplot2 code creating your plot! Copy the code and paste it into a script to continue working on it.\n4. ggplot2 Code esquisse is a helpful shortcut in getting started, but you will almost always need to do some fine-tuning of the resulting code. The first step of that however is understanding all the component parts.\nggplot builds plots in layers. It combines those layers using a (completely unique, not used for anything else) syntax which uses the + sign to combine layers. The most common layers are as follows, and you should be able to see them in your esquisse output.\nggplot(\u0026lt;DATA\u0026gt;) + Here is where you define the dataframe you are working with. You can use the column names from this dataframe in the rest of the ggplot code without using the dataframe name and $. aes(x = \u0026lt;VARIABLE\u0026gt;, y = \u0026lt;VARIABLE\u0026gt;, color = \u0026lt;VARIABLE\u0026gt;) + The aes() or \u0026ldquo;aesthetic mappings\u0026rdquo; tell ggplot what variables belong where. These are the elements boxes we see in esquisse. You can either define the aes alone, in which case it will use the same variables for all layers of the plot. Alternatively, you can define the aes for a specific geom_XXXX() layer as we will see next. geom_jitter(size = \u0026lt;VALUE\u0026gt;) + Next up comes out geom_XXXX() layers. One geom defines one type of plot to layer on. For example, here we have a geom_jitter() which adds a jittered scattered plot layer. We could also use a geom_bar() for a bar plot, a geom_histogram() for a histogram, etc. We could define aes inside a geom if we wanted, instead of outside like we did before, in which case the data would only apply for that layer. We could thus theoretically layer on multiple datasets in one plot. labs() + The labs() function lets us add labels, titles, and captions to our plot. You will usually at least want to add the title = \u0026lt;CHARACTER\u0026gt;, x = \u0026lt;CHARACTER\u0026gt;, and y = \u0026lt;CHARACTER\u0026gt; arguments. theme_minimal() ggplot has a number of pre-defined themes you can use. You can find the one you like most, but I typically use theme_minimal() as it cuts away everything that isn\u0026rsquo;t useful. All of these elements build up to something that will look about like this:\nggplot(nbi_hampshire) + aes( x = YEAR_BUILT_027, y = STRUCTURAL_EVAL_067, color = ROUTE_PREFIX_005B_L ) + geom_jitter(size = 1.5) + labs(title = \u0026#34;Vehicle Bridges in Hampshire County\u0026#34;, x = \u0026#34;Year\u0026#34;, y = \u0026#34;Structural Evaluation Score\u0026#34;, color = \u0026#34;Type\u0026#34;) + theme_minimal() Create a box plot using ggplot/esquisse which shows STRUCTURAL_EVAL_067 by MAINTENANCE_021_L.\nSomething like ggplot(nbi_hampshire) + aes(x = MAINTENANCE_021_L, y = STRUCTURAL_EVAL_067) + geom_boxplot(fill = \u0026lsquo;#112446\u0026rsquo;) + theme_minimal()\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/13_dyn_plot/","title":"Dynamic Plotting","tags":[],"description":"","content":" Overview The Data Problem Sets 0. Make a Plot 1. Adding Labels to a ggplot 2. Isolating Elements 3. Make it Interactive 4. Make it Accessible 5. Try Something New Overview We have now learned how to make some nicer looking plots, but there is still room for improvement. All plots should tell a story, and you can use specific elements to really draw a viewers attention to that story. Today I\u0026rsquo;ll be showing a few tools to do that. You won\u0026rsquo;t need everything you see today for every plot, but they are important tools to have in your toolbox. Note that while I will be using a scatter plot throughout the worksheet today, the same tricks can be applied to any kind of plot.\nThe Data Today we will be using data from the Environmental Protection Agency (EPA) Fuel Economy data. It comes with the ggplot2 package, and we can load it into the environment using the following (only after you have loaded ggplot2):\nlibrary(ggplot2) mpg = data.frame(mpg) Here is a rundown of what the variables are:\nmanufacturer manufacturer name model model name displ engine displacement, in liters year year of manufacture cyl number of cylinders trans type of transmission drv the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty city miles per gallon hwy highway miles per gallon fl fuel type class \u0026ldquo;type\u0026rdquo; of car Problem Sets 0. Make a Plot Before we start making a fancy plot, we need a basic plot. I will provide the code for one here so everyone is starting from the same place.\nggplot(mpg) + aes(x = displ, y = hwy) + geom_point(size = 1.5) + labs( x = \u0026#34;Engine Displacement\u0026#34;, y = \u0026#34;Highway MPG\u0026#34;, title = \u0026#34;Highway MPG by Engine Size\u0026#34;, caption = \u0026#34;Source: https://fueleconomy.gov/\u0026#34; ) + theme_minimal() + theme(plot.title = element_text(face = \u0026#34;bold\u0026#34;)) We will be modifying this plot for the remainder of the worksheet. Make sure you understand all the parts!\n1. Adding Labels to a ggplot In plot terminology, labels are text we add to a plot to explain some element; we add labels to the axes for example. We can also add labels to the data itself if it will help us tell our story. It\u0026rsquo;s easy to go too far though, so this is often used sparingly.\nThere are several methods to add labels, but I\u0026rsquo;m going to show you the most general one. We start by adding a new column to our mpg dataframe, which we\u0026rsquo;ll call labels.\nAdd a new labels column to our mpg dataframe. For now, just copy the model column.\nmpg$labels = mpg$model\nLet\u0026rsquo;s see how we can incorporate these labels.\nModify the plot above by adding a new argument to the aes() function called label, and provide it our new labels column. Then, add a new gemom, geom_label(), to the ggplot code. In the label geom, include the argument vjust = -1 to move the labels a bit so they aren\u0026rsquo;t right on top of our points (which makes them hard to read).\nggplot(mpg) + aes(x = displ, y = hwy, label = labels) + geom_point(size = 1.5) + labs( x = \u0026ldquo;Engine Displacement\u0026rdquo;, y = \u0026ldquo;Highway MPG\u0026rdquo;, title = \u0026ldquo;Highway MPG by Engine Size\u0026rdquo;, caption = \u0026ldquo;Source: https://fueleconomy.gov/\u0026quot; ) + geom_label(vjust = -1) + theme_minimal() + theme(plot.title = element_text(face = \u0026ldquo;bold\u0026rdquo;))\nIf all went according to plan, we should have labels! Not the nicest to look at though. We need to be selective about where we add our labels. Let\u0026rsquo;s cut back a bit.\nFill the lables column with NAs. Then, use sub-setting to add a label only for rows where the class is \u0026ldquo;midsize.\u0026rdquo;\nmpg$labels = NA\nmpg$labels = ifelse(mpg$class == \u0026ldquo;midsize\u0026rdquo;, \u0026ldquo;Midsize\u0026rdquo;, NA)\n2. Isolating Elements We\u0026rsquo;re getting closer. Adding text to a plot can he helpful, but only for a small handful of points. If we want to highlight a whole group of points, using something like color is often more helpful. Let\u0026rsquo;s try using color to highlight our midsize cars. We can do this by adding another layer on top of our current points, but only for those we want to highlight.\nFirst we need to remove the label argument and label geom we added in the last section. Next, I add a new geom_point() below the one we already have. If we are thinking in terms of layers, adding a new geom below the other will place this layer on top of the regular points. I\u0026rsquo;m going to highlight the cars in the data where the model is a \u0026ldquo;corvette\u0026rdquo; (a famous sports car). I also added a subtitle to explain the highlights. You can see the results below:\nggplot(mpg) + aes(x = displ, y = hwy) + geom_point(size = 1.5) + # This geom is new, and let\u0026#39;s us highlight specific points geom_point(data = mpg[mpg$model == \u0026#34;corvette\u0026#34;,], aes(x = displ, y = hwy), color = \u0026#34;red\u0026#34;, size = 3) + labs( x = \u0026#34;Engine Displacement\u0026#34;, y = \u0026#34;Highway MPG\u0026#34;, title = \u0026#34;Highway MPG by Engine Size\u0026#34;, # add subtitle to explain highlights subtitle = \u0026#34;Corvettes Highlighted in Red\u0026#34;, caption = \u0026#34;Source: https://fueleconomy.gov/\u0026#34; ) + theme_minimal() + theme(plot.title = element_text(face = \u0026#34;bold\u0026#34;)) Pause to make sure you understand the modifications I made to create this plot.\nNow our plot highlights the corvettes in our data with a specific color! Let\u0026rsquo;s go one step further and add a call-out for those highlights. Previously we added labels to a set of points, but we can also arbitrarily add text as a new layer in ggplot using the annotate() function like the following:\nggplot(mpg) + aes(x = displ, y = hwy) + geom_point(size = 1.5) + # This geom is new, and let\u0026#39;s us highlight specific points geom_point(data = mpg[mpg$model == \u0026#34;corvette\u0026#34;,], aes(x = displ, y = hwy), color = \u0026#34;red\u0026#34;, size = 3) + labs( x = \u0026#34;Engine Displacement\u0026#34;, y = \u0026#34;Highway MPG\u0026#34;, title = \u0026#34;Highway MPG by Engine Size\u0026#34;, # add subtitle to explain highlights subtitle = \u0026#34;Corvettes Highlighted in Red\u0026#34;, caption = \u0026#34;Source: https://fueleconomy.gov/\u0026#34; ) + # add annotation annotate(geom = \u0026#34;label\u0026#34;, x = 6.25, y = 28, label = \u0026#34;Corvettes\u0026#34;, color = \u0026#34;red\u0026#34;) + theme_minimal() + theme(plot.title = element_text(face = \u0026#34;bold\u0026#34;)) Such call-outs can be a great way to help tell the story in your data. In this case we are showing how the high-powered sports cars (corvettes) seem to stand as outliers from the trend we see for most other cars.\nModify the code above to highlight and annotate another model of car.\nVaries.\n3. Make it Interactive Making an interactive plot can be a complex task and often requires a mix of data science and web development skills. We are going to be taking the easier route and relying on our new-found ggplot capabilities. Using the plotly package in R, we can quickly turn ggplots into interactive versions of themselves.\nFirst we will need to install plotly:\ninstall.packages(\u0026#34;plotly\u0026#34;) After that we can load in in using library():\nlibrary(plotly) plotly relies on the same layer-based logic as ggplot, but is slightly different. It is completely possible to build a plotly plot from scratch like a ggplot. For example:\nplot_ly(mpg, type = \u0026#34;scatter\u0026#34;, mode = \u0026#34;markers\u0026#34;) |\u0026gt; add_markers(x = mpg$displ, y = mpg$hwy) |\u0026gt; layout(title = \u0026#34;Highway MPG by Engine Size\u0026#34;, xaxis = list(title = \u0026#34;Engine Displacement\u0026#34;), yaxis = list(title = \u0026#34;Highway MPG\u0026#34;)) However, it also has a very helpful function called ggplotly() which we can wrap around our ggplot to convert it automatically:\nmpg_ggplot = ggplot(mpg) + aes(x = displ, y = hwy) + geom_point(size = 1.5) + labs( x = \u0026#34;Engine Displacement\u0026#34;, y = \u0026#34;Highway MPG\u0026#34;, title = \u0026#34;Highway MPG by Engine Size\u0026#34;, caption = \u0026#34;Source: https://fueleconomy.gov/\u0026#34; ) + theme_minimal() + theme(plot.title = element_text(face = \u0026#34;bold\u0026#34;)) ggplotly(mpg_ggplot) Ta-Da! Quick and easy interactive plot. By default we can hover over the data points to get a readout of where the points sit on our X and Y axis. We can also add custom information using a little trickery. We can add a fake aesthetic in our aes() called \u0026ldquo;text\u0026rdquo; and write a custom message for our pop-ups. This will rely on some information about working with text and HTML that we won\u0026rsquo;t cover until later, but I want you to know it is possible.\nmpg_ggplot = ggplot(mpg) + aes(x = displ, y = hwy, text = paste0(\u0026#34;CAR INFO:\u0026lt;/br\u0026gt;\u0026lt;/br\u0026gt;\u0026#34;, \u0026#34;Engine Displacement: \u0026#34;, displ, \u0026#34;\u0026lt;/br\u0026gt;Highway MPG: \u0026#34;, hwy, \u0026#34;\u0026lt;/br\u0026gt;Model: \u0026#34;, model) ) + geom_point(size = 1.5) + labs( x = \u0026#34;Engine Displacement\u0026#34;, y = \u0026#34;Highway MPG\u0026#34;, title = \u0026#34;Highway MPG by Engine Size\u0026#34;, caption = \u0026#34;Source: https://fueleconomy.gov/\u0026#34; ) + theme_minimal() + theme(plot.title = element_text(face = \u0026#34;bold\u0026#34;)) ggplotly(mpg_ggplot, tooltip = \u0026#34;text\u0026#34;) That\u0026rsquo;s handy. The same process can work for most plot types we have covered so far in class.\nUse the ggplotly() function to make your own custom plot from above (with the new highlights on some class of car) interactive!\nVaries.\n4. Make it Accessible The last thing I want us to think about regarding out plots is accessibility. Accessibility broadly is an area of study dedicated to making sure people of all different abilities can equally access your content. Many people make it their whole career to study good accessibility practices (like our own Dr.Â Cao in SDS!). Today I will give you a few things to keep in mind while making your plots which move you toward best practices.\nColor Blindness First is to be aware of your color choices. There are many different ways in which the colors you choose may be difficult for some people to interpret. We want to make sure our color palette is as clear as possible for everyone. A tool I like to help with this is Viz Palette. It will help you build a color palette, and then see what it would look like if you had various difficulties seeing color.\nResolution Not only do high resolution plots look better, they are also easier to understand for people with vision difficulties. There are some tricks in R for getting nice crisp plots. The first is to export them in a \u0026ldquo;vector\u0026rdquo; rather than \u0026ldquo;raster\u0026rdquo; format. A vector format essentially converts your plot into a long formula which a computer can use to draw the plot no matter what size it is. This is compared to a raster format which has a certain number of pixels, or dots, and it can never be any larger than that.\nThere aren\u0026rsquo;t many tricks for this, aside from this: when possible, export your static (non-interactive) plots as an SVG or PDF file. These are both options when you click the \u0026ldquo;Export\u0026rdquo; button in the plots pane to the right. Just know that sometimes whatever service you are using won\u0026rsquo;t accept them, and you\u0026rsquo;ll need to use something else.\nInteractive plots are already math based, and should always be able to scale. They are far less compatible with many systems, however.\nAlt Text Whenever you are working online, it is possible to include certain metadata along with your plots. One of the most important you can include is called \u0026ldquo;Alt Text.\u0026rdquo; This text is typically a description of whatever the thing is it is attached to. This alt text is used by software called screen readers, which will use the text to describe the thing to someone who cannot see the screen.\nWhenever you are authoring a R Markdown or quarto document, you can include alt text for the images you include. For example, you can include an image using the following syntax:\n![Cpation for image](./path/to/imag.svg \u0026quot;Alt text that describes the image\u0026quot;) For example, try hovering your mouse over this plot for a few seconds:\n![Alt Text Example](img/alt_text.png \u0026quot;This plot shows engine displacement on the X axis compared against the highway miles per gallon on the Y axis. Corvette cars are outliers wither higer miles per gallon despire their larg engines\u0026quot;) Alt Text Example 5. Try Something New For the last part of this worksheet, take a look at our mpg dataframe and pick some aspect of it you would like to visualize. I encourage you to look at From Data to Vis and use the decicion tree there to help pick what plot type fits the data. Once you have decided on a plot type, click on its icon, read through its desciprtion, and then scroll down to the \u0026ldquo;R Graph Gallery\u0026rdquo; to see some ways you can make it in R. If you feel it is a good fit for the data, try to make one! If not, go back and pick a different type.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/15_ethics/","title":"Data Science Ethics","tags":[],"description":"","content":" Overview Codes of Ethics Data Science Oath Data Values and Principles Case Study Overview Today we will be considering some of the ethical dilemmas data scientists encounter in their work. Failure to consider the ethical implications of our work can result in truly disastrous consequences. From manipulating the political process1, criminalizing poor parents and removing their children2, all the way to skewing the societal view of race and crime3, data systems have massive potential for harm. It is thus the duty of every data scientists to think carefully about their work, and try to prevent creating the next monster system.\nGetting as sense of ethics in data science is not a simple matter. It requires understanding not only the pressures on you during your work, but the hypothetical consequences of that work. That\u0026rsquo;s not easy even for the most experienced among us. It is also fully possible for reasonable people to disagree on both what should be considered generally under the umbrella of data science ethics, and the best course of action to resolve any individual problem.\nCodes of Ethics Ethics, broadly defined, are the framework with which we decide what is right and wrong. As you might imagine, there is a significant amount of variation between people in their sense of ethics. Yet most people can often find some common ground.\nTo help people think through the potential dangers of their work, several organizations have attempted to create a code of ethics for data scientists. These codes attempt to create a framework what is considered ethical within a specific discipline. I will be including two such examples here: the National Academies of Sciences Data Science Oath, and the datapractices.org Data Values and Principles Manifesto.\nData Science Oath The National Academies of Sciences attempted to create an oath similar to the hippocratic oath the doctors take for data scientists. This is my favorite attempt to codify data science ethics and I encourage you to mediate on it deeply.\nI swear to fulfill, to the best of my ability and judgment, this covenant: I will respect the hard-won scientific gains of those data scientists in whose steps I walk and gladly share such knowledge as is mine with those who follow. I will apply, for the benefit of society, all measures which are required, avoiding misrepresentations of data and analysis results. I will remember that there is art to data science as well as science and that consistency, candor, and compassion should outweigh the algorithm\u0026rsquo;s precision or the interventionist\u0026rsquo;s influence. I will not be ashamed to say, \u0026ldquo;I know not,\u0026rdquo; nor will I fail to call in my colleagues when the skills of another are needed for solving a problem. I will respect the privacy of my data subjects, for their data are not disclosed to me that the world may know, so I will tread with care in matters of privacy and security. If it is given to me to do good with my analyses, all thanks. But it may also be within my power to do harm, and this responsibility must be faced with humbleness and awareness of my own limitations. I will remember that my data are not just numbers without meaning or context, but represent real people and situations, and that my work may lead to unintended societal consequences, such as inequality, poverty, and disparities due to algorithmic bias. My responsibility must consider potential consequences of my extraction of meaning from data and ensure my analyses help make better decisions. I will perform personalization where appropriate, but I will always look for a path to fair treatment and nondiscrimination. I will remember that I remain a member of society, with special obligations to all my fellow human beings, those who need help and those who don\u0026rsquo;t. If I do not violate this oath, may I enjoy vitality and virtuosity, respected for my contributions and remembered for my leadership thereafter. May I always act to preserve the finest traditions of my calling and may I long experience the joy of helping those who can benefit from my work.\nNational Academies of Sciences. (2018). Data Science for Undergraduates: Opportunities and Options.\nData Values and Principles datapractices.org compiled a list of 12 principles they feel provides strong guidance for working ethically in data science. The principles serve as solid guidance for conscientious work, but I caution against seeing them as a \u0026ldquo;project checklist,\u0026rdquo; such that if your project matches all the principles it gets an \u0026ldquo;ethical\u0026rdquo; stamp of approval.\nUse data to improve life for our users, customers, organizations, and communities. Create reproducible and extensible work. Build teams with diverse ideas, backgrounds, and strengths. Prioritize the continuous collection and availability of discussions and metadata. Clearly identify the questions and objectives that drive each project and use to guide both planning and refinement. Be open to changing our methods and conclusions in response to new knowledge. Recognize and mitigate bias in ourselves and in the data we use. Present our work in ways that empower others to make better-informed decisions. Consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society. Respect and invite fair criticism while promoting the identification and open discussion of errors, risks, and unintended consequences of our work. Protect the privacy and security of individuals represented in our data. Help others to understand the most useful and appropriate applications of data to solve real-world problems. Data Values and Principles Manifesto\nCase Study We will be going over the following case study as a group. Please read the scenario carefully, and then write down your own reaction to it. Please consider the following:\nBroadly, do you think this is ethical? Are any laws broken? What role would a data scientist have in this process? How would you apply one or both of the ethical codes included above to this situation? In the United States, most students apply for grants or subsidized loans to finance their college education. Part of this process involves filling in a federal government form called the Free Application for Federal Student Aid (FAFSA). The form asks for information about family income and assets. The form also includes a place for listing the universities to which the information is to be sent. The data collected by FAFSA includes confidential financial information (listing the schools eligible to receive the information is effectively giving permission to share the data with them). It turns out that the order in which the schools are listed carries important information. Students typically apply to several schools, but can attend only one of them. Until recently, admissions offices at some universities used the information as an important part of their models of whether an admitted student will accept admissions. The earlier in a list a school appears, the more likely the student is to attend that school. Here\u0026rsquo;s the catch from the student\u0026rsquo;s point of view. Some institutions use statistical models to allocate grant aid (a scarce resource) where it is most likely to help ensure that a student enrolls. For these schools, the more likely a student is deemed to accept admissions, the lower the amount of grant aid they are likely to receive.\nFrom Baumer, B. S., Kaplan, D. T., \u0026amp; Horton, N. J. (2021). Chapter 8 Data science ethics. In Modern Data Science with R. CNC Press.\nO\u0026rsquo;Neil, C. (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. St.Â Martin\u0026rsquo;s Press.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNoble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. New York University Press.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/17_functions/","title":"Functions","tags":[],"description":"","content":" Overview The Data Functions Under the Hood Solving a Problem Make it General Try it Yourself Overview Functions are the backbone of how R does things; we have refereed to them previously as \u0026ldquo;verbs\u0026rdquo; in the language of R. We have used several functions so far, either from base R, or from packages we have loaded in using library(). However, those are not all the functions available to us. The true power in R comes from our ability to make our own functions to do anything we want. That\u0026rsquo;s what we\u0026rsquo;ll be practicing today.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) Functions Under the Hood We can peek behind the curtain on any of the functions we have used so far an see how they tick. All you have to do is enter the function into the console, and remove the () where the arguments go. Let\u0026rsquo;s look at our friend table() by typing table into the console and hitting enter. You should see the following:\nfunction (..., exclude = if (useNA == \u0026#34;no\u0026#34;) c(NA, NaN), useNA = c(\u0026#34;no\u0026#34;, \u0026#34;ifany\u0026#34;, \u0026#34;always\u0026#34;), dnn = list.names(...), deparse.level = 1) { list.names \u0026lt;- function(...) { l \u0026lt;- as.list(substitute(list(...)))[-1L] if (length(l) == 1L \u0026amp;\u0026amp; is.list(..1) \u0026amp;\u0026amp; !is.null(nm \u0026lt;- names(..1))) return(nm) nm \u0026lt;- names(l) fixup \u0026lt;- if (is.null(nm)) seq_along(l) else nm == \u0026#34;\u0026#34; dep \u0026lt;- vapply(l[fixup], function(x) switch(deparse.level + 1, \u0026#34;\u0026#34;, if (is.symbol(x)) as.character(x) else \u0026#34;\u0026#34;, deparse(x, nlines = 1)[1L]), \u0026#34;\u0026#34;) if (is.null(nm)) dep else { nm[fixup] \u0026lt;- dep nm } } miss.use \u0026lt;- missing(useNA) miss.exc \u0026lt;- missing(exclude) useNA \u0026lt;- if (miss.use \u0026amp;\u0026amp; !miss.exc \u0026amp;\u0026amp; !match(NA, exclude, nomatch = 0L)) \u0026#34;ifany\u0026#34; else match.arg(useNA) doNA \u0026lt;- useNA != \u0026#34;no\u0026#34; if (!miss.use \u0026amp;\u0026amp; !miss.exc \u0026amp;\u0026amp; doNA \u0026amp;\u0026amp; match(NA, exclude, nomatch = 0L)) warning(\u0026#34;\u0026#39;exclude\u0026#39; containing NA and \u0026#39;useNA\u0026#39; != \\\u0026#34;no\\\u0026#34;\u0026#39; are a bit contradicting\u0026#34;) args \u0026lt;- list(...) if (length(args) == 1L \u0026amp;\u0026amp; is.list(args[[1L]])) { args \u0026lt;- args[[1L]] if (length(dnn) != length(args)) dnn \u0026lt;- paste(dnn[1L], seq_along(args), sep = \u0026#34;.\u0026#34;) } if (!length(args)) stop(\u0026#34;nothing to tabulate\u0026#34;) bin \u0026lt;- 0L lens \u0026lt;- NULL dims \u0026lt;- integer() pd \u0026lt;- 1L dn \u0026lt;- NULL for (a in args) { if (is.null(lens)) lens \u0026lt;- length(a) else if (length(a) != lens) stop(\u0026#34;all arguments must have the same length\u0026#34;) fact.a \u0026lt;- is.factor(a) if (doNA) aNA \u0026lt;- anyNA(a) if (!fact.a) { a0 \u0026lt;- a op \u0026lt;- options(warn = 2) a \u0026lt;- factor(a, exclude = exclude) options(op) } add.na \u0026lt;- doNA if (add.na) { ifany \u0026lt;- (useNA == \u0026#34;ifany\u0026#34;) anNAc \u0026lt;- anyNA(a) add.na \u0026lt;- if (!ifany || anNAc) { ll \u0026lt;- levels(a) if (add.ll \u0026lt;- !anyNA(ll)) { ll \u0026lt;- c(ll, NA) TRUE } else if (!ifany \u0026amp;\u0026amp; !anNAc) FALSE else TRUE } else FALSE } if (add.na) a \u0026lt;- factor(a, levels = ll, exclude = NULL) else ll \u0026lt;- levels(a) a \u0026lt;- as.integer(a) if (fact.a \u0026amp;\u0026amp; !miss.exc) { ll \u0026lt;- ll[keep \u0026lt;- which(match(ll, exclude, nomatch = 0L) == 0L)] a \u0026lt;- match(a, keep) } else if (!fact.a \u0026amp;\u0026amp; add.na) { if (ifany \u0026amp;\u0026amp; !aNA \u0026amp;\u0026amp; add.ll) { ll \u0026lt;- ll[!is.na(ll)] is.na(a) \u0026lt;- match(a0, c(exclude, NA), nomatch = 0L) \u0026gt; 0L } else { is.na(a) \u0026lt;- match(a0, exclude, nomatch = 0L) \u0026gt; 0L } } nl \u0026lt;- length(ll) dims \u0026lt;- c(dims, nl) if (prod(dims) \u0026gt; .Machine$integer.max) stop(\u0026#34;attempt to make a table with \u0026gt;= 2^31 elements\u0026#34;) dn \u0026lt;- c(dn, list(ll)) bin \u0026lt;- bin + pd * (a - 1L) pd \u0026lt;- pd * nl } names(dn) \u0026lt;- dnn bin \u0026lt;- bin[!is.na(bin)] if (length(bin)) bin \u0026lt;- bin + 1L y \u0026lt;- array(tabulate(bin, pd), dims, dimnames = dn) class(y) \u0026lt;- \u0026#34;table\u0026#34; y } That\u0026rsquo;s pretty complex looking; and it is. However, the point is that this is the same code that runs when we call table. We could, it we wanted, type this out ourselves and create our own table function using the\u0026hellip; function() function, like we see at the top the the previous code.\nCopy the code from the table() function, and assign it to a new function called my_table(). Run both of them on the mint_choc column from our survey data. Does the output make sense?\nmy_table = table\ntable(survey$mint_choc) my_table(survey$mint_choc)\nNot all functions are as transparent. Try looking at the code behind the sum() function and you may be disappointed. The most basic functions in R are \u0026ldquo;primitives,\u0026rdquo; and are actually calling a lower level programming language than we can access through R. Looking at sum we see:\nfunction (..., na.rm = FALSE) .Primitive(\u0026#34;sum\u0026#34;) You may also be surprised to learn that pretty much everything in R is a function, though often a primitive one. For example, we can look at the + sign as a function by warping it in ticks like so:\n`+` function (e1, e2) .Primitive(\u0026quot;+\u0026quot;) Neat.\nSolving a Problem It\u0026rsquo;s cool that we can make a copy of table, but why would we? We wouldn\u0026rsquo;t really. What we would do is create our own functions. While there a ton of packages out there to do all sorts of things, sometimes you will just need to make something yourself. This is often the case when working with your own unique data sets.\nLet\u0026rsquo;s take a look at our survey data, specifically the pets column. As you can see below, we have some issues that violate our tidy data principles. Specifically, in our one column of pets, we have values on multiple types of pets. Ideally, we would like a column for each type of pet, and then a TRUE or FALSE for if people had that kind of pet.\nsurvey$pets [1] \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; \u0026quot;Dog, Plants (two)\u0026quot; [4] \u0026quot;None\u0026quot; \u0026quot;Dog\u0026quot; \u0026quot;Dog\u0026quot; [7] \u0026quot;Cat, Rock\u0026quot; \u0026quot;Dog\u0026quot; \u0026quot;Spider Plant\u0026quot; [10] \u0026quot;Cat\u0026quot; \u0026quot;Dog, Reptile\u0026quot; \u0026quot;Dog\u0026quot; [13] \u0026quot;Cat\u0026quot; \u0026quot;None\u0026quot; \u0026quot;Reptile, Plant\u0026quot; Here\u0026rsquo;s where a custom function can help us. We know that each of the entries in pets is separated by a comma, and we can exploit that structure. We\u0026rsquo;re going to write a function that will split those values for us by commas, and create a new pets dataframe that we can then attach to our current survey dataframe.\nThe first step of creating a good function is clearly defining the key components: the input, output, and arguments. In our case, the input will be our pets column, the output will be a dataframe with one column per pet type, and a value of TRUE or FALSE depending on if the case had that pet. We won\u0026rsquo;t need any arguments aside from the input for now.\nWhen I am creating a function, I typically write the code to do what I want first, and then convert it into a function. Let\u0026rsquo;s write some code to create our output dataframe, and then we can work to fill it. I want to stress that this is one way to go about solving this problem. There are plenty of other valid (and easier, once we learn some more skills) options.\nFirst, we\u0026rsquo;ll create a new dataframe with a column for each of our possible pets, and a row for all 15 of our cases. I\u0026rsquo;ll fill the dataframe with NAs for now.\npet_output = data.frame( \u0026#34;id\u0026#34; = 1:15, \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) Now, we need to figure out a way to test if the person listed one of our options in their response. We\u0026rsquo;ll use a new function called grepl() to test this. The name \u0026ldquo;grepl\u0026rdquo; is a product of very old programmer speak, but practically it will search through a character vector and give us a TRUE or FALSE if it finds a match for the pattern we give it. Thus, we can run the following on our pets column for each of our possible pets to test if they are included. We\u0026rsquo;re going to tell it to ignore case, so that capital and lower case letters don\u0026rsquo;t matter.\nRun the following code and explain it\u0026rsquo;s output.\ngrepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) It says TRUE if the value included \u0026lsquo;dog\u0026rsquo; and FALSE if it did not.\nWe can use the same function to test for each of our possible pets, and assign the results to our new dataframe.\npet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE NA 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE NA 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE NA Now, that works for everything except \u0026ldquo;other.\u0026rdquo;\nWhy would we need a different strategy for the \u0026ldquo;other\u0026rdquo; cases?\nWe don\u0026rsquo;t know what will be in \u0026lsquo;other\u0026rsquo; so we can\u0026rsquo;t test explicitly for it.\nTo resolve our \u0026ldquo;other\u0026rdquo; issue, we will need to take another approach. First, we\u0026rsquo;ll make a copy of our pets column we can work with, and make sure we don\u0026rsquo;t alter our original data.\nwork_pets = survey$pets Next, we will look through that copy, and remove everything that fits into one of our other categories using gsub(), or \u0026ldquo;general substitution.\u0026rdquo; gsub() is similar to grepl() in that it asks for a pattern to look for and where to look for it, but also asks what to substitute for that pattern. Let\u0026rsquo;s look at the following example. Here I ask gsub() to look in our work_pets vector, find all the times \u0026ldquo;dog\u0026rdquo; appears, and to replace it with \u0026ldquo;\u0026rdquo;, or nothing.\ngsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) [1] \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; \u0026quot;, Plants (two)\u0026quot; \u0026quot;None\u0026quot; [5] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Cat, Rock\u0026quot; \u0026quot;\u0026quot; [9] \u0026quot;Spider Plant\u0026quot; \u0026quot;Cat\u0026quot; \u0026quot;, Reptile\u0026quot; \u0026quot;\u0026quot; [13] \u0026quot;Cat\u0026quot; \u0026quot;None\u0026quot; \u0026quot;Reptile, Plant\u0026quot; We can see in the output that now all the instances of \u0026ldquo;dog\u0026rdquo; have been removed. We will repeat this process for each of our known categories, each time saving our results back to work_pets. We will also remove commas, and use trimws(), or \u0026ldquo;trim white space,\u0026rdquo; to delete extra spaces from the start and end of our characters.\nwork_pets = gsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;cat\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;fish\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;bird\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;reptile\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;rock\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;none\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;,\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = trimws(work_pets) work_pets [1] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Plants (two)\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; [6] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Spider Plant\u0026quot; \u0026quot;\u0026quot; [11] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Plant\u0026quot; If we look at work_pets now, all that is left are things not in our pets categories. We can now either turn this into a logical, or assign these values to our new other column in pet_output. I\u0026rsquo;ll do the latter. I\u0026rsquo;ll also convert our blanks into proper NAs.\npet_output$other = work_pets pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA pet_output id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE Plants (two) 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Spider Plant 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE Plant If we look at out pet_output dataframe now, we can see we have a column for each pet type, a TRUE or FALSE for each known pet, and the text for other pets. All in all, the code to do this looks like:\n# create an empty dataframe for our intended output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:15, \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = survey$pets, ignore.case = TRUE) # make a copy of the pets vector to work on work_pets = survey$pets # remove all known pets and clean remaining text work_pets = gsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;cat\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;fish\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;bird\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;reptile\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;rock\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;none\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;,\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = trimws(work_pets) # Fill in \u0026#34;other\u0026#34; pet_output$other = work_pets # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA Make it General Now that we know what to do, let\u0026rsquo;s get to work converting this into a function. The key advantage of doing so is that we can make out code more generalizable. Rather than hard-coding each element, we can instead use variables for stand-ins that can be swapped later on. For example, instead of performing all of these checks on survey$pets, we can write the function to look at any pets vector we pass as an argument. This means we can re-use the function later!\nTo start this process, let\u0026rsquo;s write a skeleton for our function. Below I\u0026rsquo;ve included a skeleton for a new function I am calling pet_split. In this function, I have an argument called pet_vector. Now, whenever I use pet_vector inside the body of the function, I will be telling R to use whatever is given to the function in the pet_vector argument. pet_vector is just like an ordinary object in your R environment, except it only exists within the function. You can think about it like R opening a little mini-R, running all the code in the function top to bottom, giving you the result, then closing it and deleting everything else inside.\nI\u0026rsquo;ll say it again because it is so important to understand. You can think about functions as if they were opening a little mini-R universe, running all the code in the function top to bottom with the arguments you provided as objects, giving you the result, then destroying that universe and deleting everything else inside.\npet_split = function(pet_vector) { } Now let\u0026rsquo;s add some substance to this function. I\u0026rsquo;ll start by adding in the code to create a dataframe for our output, and a return() at the end. Whatever I put inside return() will be the result of the function when it is run, and everything else will be deleted when the mini-R inside the function is closed. I also changed the code of our \u0026ldquo;id\u0026rdquo; column a bit. Instead of hard-coding 15 IDs, I made it more generalizable by asking R to make IDs 1 through the number of elements, or the length, of our pet_vector argument. That means the dataframe will always have the same number of rows as the pet_vector input, no matter how many elements it has.\nThe only thing that will come out of your function is whatever you put in the return() function. Messages (like print()) are exceptions.\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 NA NA NA NA NA NA NA NA 2 2 NA NA NA NA NA NA NA NA 3 3 NA NA NA NA NA NA NA NA 4 4 NA NA NA NA NA NA NA NA 5 5 NA NA NA NA NA NA NA NA 6 6 NA NA NA NA NA NA NA NA 7 7 NA NA NA NA NA NA NA NA 8 8 NA NA NA NA NA NA NA NA 9 9 NA NA NA NA NA NA NA NA 10 10 NA NA NA NA NA NA NA NA 11 11 NA NA NA NA NA NA NA NA 12 12 NA NA NA NA NA NA NA NA 13 13 NA NA NA NA NA NA NA NA 14 14 NA NA NA NA NA NA NA NA 15 15 NA NA NA NA NA NA NA NA Next, let\u0026rsquo;s get this function to output something we actually want. I\u0026rsquo;ll copy more of our code from above into the function. You\u0026rsquo;ll notice that I replace any calls for survey$pets with our generic argument pet_vector. If we run this version, we start to see our desired output!\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE NA 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE NA 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE NA To finish it up, let\u0026rsquo;s copy the rest of our code into this function. You\u0026rsquo;ll notice that I don\u0026rsquo;t need to make a new vector work_pets as before. That\u0026rsquo;s because I\u0026rsquo;m working on the pet_vector object directly.\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE Plants (two) 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Spider Plant 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE Plant Our new pet_split function works! It will create a new dataframe we can combine with our survey data to get tidy data about pets. Now we could save this function somewhere, and re-use it on every survey with a question about pets without having to re-code all of those steps each time.\nTake some time to study the above function. Make sure you understand all of the steps it is taking, and how it produces the output that it does.\nNow, we could make this even more general. In reality here we are solving the problem of splitting values by commas; several of our columns have that problem! We will go over how to make this function even better next week when we learn about iteration.\nTry it Yourself Let\u0026rsquo;s try creating a function on your own now.\nCreate a function that will intake a ROW from the survey dataframe, and output a number showing how many total NAs there were in there row. The is.na() function will come in handy here. An example input and output should look like:\ntotal_na(survey_row = survey[1,]) OUTPUT: 2 total_na = function(survey_row) {\n# get the sum of NAs output = sum(is.na(survey_row))\n# output return(output) }\ntotal_na(survey_row = survey[1,])\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/18_condition_debug/","title":"Debugging and Conditions","tags":[],"description":"","content":" Overview Our Toolbox Debugging Example Puzzle 1 Puzzle 2 Overview Code will hardly ever work exactly as you want the first try. Especially early on, coding is an exercise in incremental improvements. Debugging, or identifying and removing \u0026ldquo;buggy\u0026rdquo; code that doesn\u0026rsquo;t work as intended, is the skill that lets us identify what is wrong so we can make those improvements.\nToday\u0026rsquo;s worksheet is presented as a series of puzzles. Each puzzle will be a function that has something wrong with it. I will provide an input, and the desired output. Your task is to use the debugging tools we learned to figure out what is wrong with the function, and correct it. I will walk through an example first so you get the idea.\nOur Toolbox We will be using two main tools for debugging, debugonce() and browser(). Each accomplishes the same thing is slightly different ways. These functions let you pause the execution of code mid-way inside a function, and see what is going inside our mini-R universes. This is very helpful, as opposed to just running code in your global environment, you can\u0026rsquo;t normally run code inside a function line-by-line to see what is happening to the data at each step. debugonce() and browser() let you do that. This is also very helpful while building new functions.\ndebugonce() accepts a function name, and the next time you run a function, it will drop you into the mini-universe of that function for you to look around. You can tell it worked because your console will change slightly.\nThe figure above shows what the browser window will look like. While in the browser, you can execute R code like normal, but there are a few differences.\nInstead of the regular \u0026gt; prompt in the R console, you will see Browse[#]\u0026gt; indicating you are in the browser. It still works mostly like the normal console, with a few extra commands. a) You can press Enter or enter the letter n to go to the next line of code. b) You can enter c to continue to the end of the function c) You can enter q to quit and leave the browser The script pane is replaced with a function inspector walking you through the function you are debugging. You normally can\u0026rsquo;t type in this window. The current line, what will be executed next time you press enter or n is highlighted. A few new buttons show up. These are the same as the commands described in #1. We can also use the browser() function to call the browser at a specific spot within a function. Simply add the browser() function anywhere inside a function you are writing and define the function again by executing it. Now, whenever you run that function, the browser will open wherever you added browser(). You will have to remove it from your function once you finish debugging.\nDebugging Example Here is an example function that needs some debugging. This one is relatively short, and you may be able to figure out the problem without debugging. This will not always be the case, as functions will routinely extend for a dozen or several dozen lines with multiple other function inside of them creating nested mini-universes. The debugging process will always be the same though: figure out what function the problem is in, then go inside and follow the process step-by-step.\nThis function is meant to accept a numeric vector, and then output the mean, median, and mode. Instead, it results in the error shown below.\nexample_vector = c(1, 2, 6, 8, 4, 2, 8, 2, 7, 10, 33) example_function = function(num_vec) { # get the mean vec_mean = mean(num_vec) # get the median vec_median = median(num_vec) # get the mode vec_mode = mode(num_vec) # create named vector for output output = c(\u0026#34;mean\u0026#34; = vec_mean, \u0026#34;median\u0026#34; = vec_median, \u0026#34;mode\u0026#34; = vec_mode) # make sure all results are numeric if(!all(is.numeric(output))){stop(\u0026#34;Not all values are numeric!\u0026#34;)} # return results return(output) } example_function(example_vector) Error in example_function(example_vector): Not all values are numeric! How would we go about fixing this? We only have one function, so we know where things must be going wrong. We\u0026rsquo;ll use debugonce() to get a peak inside. Copy the above function code into your console and execute it to add the function to your environment. Run example_function() on example_vector to make sure you are getting the same output as we did here.\nOnce you have done that, run debugonce(example_function), then run example_function(example_vector) again. You will be dropped into the browser, looking around inside example_function(). Step through the code execution one line at a time by pressing the Enter key. Watch the environment pane each step of the way and see if you can catch where the error will happen. Once you get in the spot in the function the error occurs, it will boot you out of the browser back to the global environment.\nAs you step through the function you should notice the at line vec_mode = mode(num_vec) produces an output of \u0026quot;numeric\u0026quot;, which would be causing our error in the next line, if(!all(is.numeric(output))){stop(\u0026quot;Not all values are numeric!\u0026quot;)}. That code asking, if all output is not (because !) numeric, then run stop().\nWe could use browser() to check that section more quickly using the following:\nexample_vector = c(1, 2, 6, 8, 4, 2, 8, 2, 7, 10, 33) example_function = function(num_vec) { # get the mean vec_mean = mean(num_vec) # get the median vec_median = median(num_vec) # -------------------------------------------------------------Browser will stop execution here. browser() # get the mode vec_mode = mode(num_vec) # create named vector for output output = c(\u0026#34;mean\u0026#34; = vec_mean, \u0026#34;median\u0026#34; = vec_median, \u0026#34;mode\u0026#34; = vec_mode) # make sure all results are numeric if(!all(is.numeric(output))){stop(\u0026#34;Not all values are numeric!\u0026#34;)} # return results return(output) } example_function(example_vector) If you re-define our example_function() using the code above then try to use it, it will always stop at the browser() function to let us look around. Try it out yourself!\nPuzzle 1 The following function will intake a vector of character names, and output a dataframe with 6 columns. The function will have each character flip a coin. If they get a heads, they can flip again, up to a max of three. If a character flips heads three times, the lucky column should be set to TRUE. Run the following several times. Every so often, a character will appear where they did not flip heads all three times, but get a TRUE in the lucky column. Fix this error.\nchar_vec = c(\u0026#34;Spike Spiegal\u0026#34;, \u0026#34;Doreamon\u0026#34;, \u0026#34;Sherlock Holmes\u0026#34;, \u0026#34;Tiana\u0026#34;, \u0026#34;Crush\u0026#34;, \u0026#34;Thor\u0026#34;, \u0026#34;Rhys\u0026#34;, \u0026#34;Buffy\u0026#34;, \u0026#34;Sasha Braus\u0026#34;, \u0026#34;Catra\u0026#34;, \u0026#34;Pikachu\u0026#34;, \u0026#34;My Melody\u0026#34;, \u0026#34;Claire Fraser\u0026#34;, \u0026#34;Shinchan\u0026#34;, \u0026#34;Kakashi\u0026#34;) puzzle_1 = function(characters) { # sort chars by alphabetical order sorted_char = sort(char_vec) # get the first letter of each name char_letters = substr(x = sorted_char, start = 1, stop = 1) # create a dataframe of character and their initial char_df = data.frame(\u0026#34;char_name\u0026#34; = sorted_char, \u0026#34;char_initial\u0026#34; = char_letters) # randomly flip a count for each char char_df$toss_1 = sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE) # for each that got heads, flip again, those with tails are out char_df$toss_2 = ifelse(char_df$toss_1 == \u0026#34;heads\u0026#34;, sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE), NA) # do it again char_df$toss_3 = ifelse(char_df$toss_1 == \u0026#34;heads\u0026#34;, sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE), NA) # add TRUE / FALSE for those with 3 heads ## set to TRUE if the 3rd toss is heads ## (as the other two had to be heads to toss a third time) char_df$lucky = ifelse(char_df$toss_3 == \u0026#34;heads\u0026#34;, TRUE, FALSE) ## fill NAs with FALSE char_df[is.na(char_df$lucky), \u0026#34;lucky\u0026#34;] = FALSE # return results return(char_df) } puzzle_1(char_vec) Fix the line:\n# do it again char_df$toss_3 = ifelse(char_df$toss_1 == 'heads', sample(x = c('heads', 'tails'), size = nrow(char_df), replace = TRUE), NA) So that it looks at toss_2 rather than toss_1.\nPuzzle 2 The following function will input our survey dataframe, and is meant to output the number of times people responded TRUE to a question. The correct output is 33, however it is currently outputting 110. Debug this function to fix the issue.\n# load data survey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) puzzle_2 = function(survey_dataframe) { # pivot the survey data from wide to long survey_long = tidyr::pivot_longer(survey_dataframe, cols = -fav_char, values_transform = as.character) # get all the questions people answered TRUE all_true = survey_long[survey_long$value == TRUE, ] # count the number of rows (the number of questions with answers of TRUE) num_true = nrow(all_true) # return that number return(num_true) } puzzle_2(survey) When we subset using:\nall_true = survey_long[survey_long$value == TRUE, ] It also includes all NAs. You can either account for the NAs like this:\nall_true = survey_long[survey_long$value == TRUE \u0026amp; !is.na(survey_long$value), ] Or subset the dataframe again like:\nall_true = survey_long[survey_long$value == TRUE, ] all_true = all_true[!is.na(all_true$value), ] "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/20_iteration/","title":"Iteration","tags":[],"description":"","content":" Overview The Data A Refresher on our pet_split() Function Break it Down Arbitrary Output Test for Each Option Remove the Known Options to Find \u0026ldquo;Other\u0026rdquo; Turn it Back into a Function Overview Iteration allows us to tell R to work on a whole sets of things at once: multiple files, multiple columns, multiple whatever. This can save quite a bit of time. It also lets us to work on problems with dependence, where the decisions of each step depends on the result of the previous step.\nFor our worksheet today, we are going to be solving some annoyances of the past. I am going to walk you though modifying our pet_split() function from last week\u0026rsquo;s functions worksheet to make it even more generalizable.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) A Refresher on our pet_split() Function Recall from last week that our pet_split() function looked at the pets column in our survey dataframe, and tidy-ed up the column so that instead of having a single character with multiple pets in it, we had a dataframe with TRUE and FALSE for each pet type, along with \u0026ldquo;other.\u0026rdquo; You can see the finished function below:\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE Plants (two) 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Spider Plant 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE Plant Now, that is cool, but we can make it better. Specifically, if we look at our survey dataframe, we have the exact same problem in our \u0026lt;DRINK\u0026gt;_days columns and the recreation column. By the end of this worksheet, our pet_split() function will work on any column with comma separated values.\nBreak it Down Our first step is going to be writing the code to accomplish what we want, then we can package it as a function. I\u0026rsquo;ve gutted our pet_split() function below. We will be starting from that base and working to make it so that we never explicitly call for anything in our code. For example, instead of coding all of the possibilities of pet (dog, cat, fish, bird, reptile, rock, none) inside the function itself, we want to write our code such that it can accept any list of possibilities as an argument and work from that.\n# set up a psudo argument pet_vector = survey$pets # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA Arbitrary Output The first step of our current code is to create a dataframe for our outputs. We still want to do that, but without us defining each possibility ourselves inside the function. Instead, we will provide a vector of possibilities, and have R iterate through those to make our columns. We can use a for() loop for that.\nFirst, we\u0026rsquo;ll create a vector of our possibilities, in this case our pets:\npossible_columns = c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;reptile\u0026#34;, \u0026#34;rock\u0026#34;, \u0026#34;none\u0026#34;) Next, we need code to iterate through those possibilities, and create a dataframe from them. We\u0026rsquo;ll start with making what we know, a column for IDs which has as many rows as our intended input, pet_vector from above. Next, we will iterate through all possible options, and make a new column for each. Here I iterate through our possible_columns vector, and for each element (option in the loop) I create a column of NAs.\n# make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all optinos and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } If we look at out output now, it is exactly the same as if we made each column ourselves, but now it is done by providing a vector of options. And we can change those options to whatever we want. This will come in handy later.\nTest for Each Option Our next step, as before, is to test for each possible option and fill in the respective columns. We will use iteration here as well.\nUsing the same principle as above, iterate over each option in possible_columns and use grepl() to test if the pet appeared in that case. Fill the respective columns. Make sure that pet_vector and possible_columns are reset to normal before you try!\nThe following code will iterate through possible_columns and replace the pattern grepl() is looking for with each option. It will test for that option, and save the results in the corresponding column.\nfor(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } Remove the Known Options to Find \u0026ldquo;Other\u0026rdquo; Once we have our \u0026ldquo;knowns\u0026rdquo; taken care of, we can work on the others. The process is nearly identical, just swap grepl() with gsub() and apply it to pet_vector like before.\nIterate over each option in possible_columns and use gsub() to remove all of our known possibilities (and commas) from pet_vector. You can then use trimsws() to remove the extra spaces. Assign the remaining values to the \u0026ldquo;other\u0026rdquo; column of pet_output.\nThe following will remove all known possibilities, clean the remainder, and assign it to the \u0026lsquo;other\u0026rsquo; column.\nfor(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = '', ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = ',', pet_vector, replacement = '', ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in 'other' pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == '', 'other'] = NA Turn it Back into a Function If we look at our code all together now, it looks like the following. If we run it, it will return the exact same thing as our old pet_split() function, but instead of each option being hand-coded by us, it knows how to work with any given vector of options and create our desired output.\n# make dummy argument pet_vector = survey$pets # set all known options possible_columns = c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;reptile\u0026#34;, \u0026#34;rock\u0026#34;, \u0026#34;none\u0026#34;) # make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = \u0026#39;,\u0026#39;, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#39;other\u0026#39; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#39;\u0026#39; \u0026amp; !is.na(pet_output$other), \u0026#39;other\u0026#39;] = NA If we turn it into a function, it will look like this:\npet_split = function(pet_vector, possible_columns){ # make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = \u0026#39;,\u0026#39;, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#39;other\u0026#39; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34; \u0026amp; !is.na(pet_output$other), \u0026#39;other\u0026#39;] = NA # return output return(pet_output) } pet_split(pet_vector = survey$pets, possible_columns = c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;reptile\u0026#34;, \u0026#34;rock\u0026#34;, \u0026#34;none\u0026#34;)) id dog cat fish bird reptile rock none other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE Plants (two) 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 6 6 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Spider Plant 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 TRUE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 14 14 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE TRUE FALSE FALSE Plant That\u0026rsquo;s pretty cool, but what do you think would happen if we tried it on another column?\npet_split(pet_vector = survey$tea_days, possible_columns = c(\u0026#34;monday\u0026#34;, \u0026#34;tuesday\u0026#34;, \u0026#34;wednesday\u0026#34;, \u0026#34;thursday\u0026#34;, \u0026#34;friday\u0026#34;, \u0026#34;saturday\u0026#34;, \u0026#34;sunday\u0026#34;)) id monday tuesday wednesday thursday friday saturday sunday other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 3 3 FALSE FALSE TRUE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 4 4 FALSE FALSE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 5 5 FALSE FALSE FALSE FALSE TRUE FALSE TRUE \u0026lt;NA\u0026gt; 6 6 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE FALSE TRUE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 8 8 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 10 10 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 14 14 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; While it now has a bit of an odd name, our function can now work on any column! It is hard to express how big of a deal that is. We now have a single general tool that can adapt itself to several situations. The input is arbitrary, as long as it is formatted the same way (values separated by commas), we can put anything through this function and get a nice tidy dataframe back. A whole new universe of possibilities just opened.\nTry our function on some other columns in the survey dataframe!\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/21_apply_list/","title":"List and Apply","tags":[],"description":"","content":" Overview The Data Apply pet_split() Working with Lists Read ALL the Files! Overview The apply family of functions can be a handy way to perform analyses and transformations of data quickly. Today we will be using both to improve some previous tasks. First we will be revisiting pet_split() (last time), and solving an old annoyance regarding reading in several data files.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = read.csv(\u0026#34;https://raw.githubusercontent.com/Intro-to-Data-Science-Template/intro_to_data_science_reader/main/content/class_worksheets/4_r_rstudio/data/survey_data.csv\u0026#34;) Apply pet_split() The first thing we will be doing today is combining our pet_split() function and lapply() in order to split all of our \u0026lt;DRINK\u0026gt;_day comma separated columns at once. I\u0026rsquo;ve provided our fully generalized version of pet_split() below. Run the following to add it to your environment.\npet_split = function(pet_vector, possible_columns){ # make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = \u0026#39;,\u0026#39;, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#39;other\u0026#39; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34; \u0026amp; !is.na(pet_output$other), \u0026#39;other\u0026#39;] = NA # return output return(pet_output) } Once you\u0026rsquo;ve got the function in your environment, we are going to use lapply() to apply it over all the relevant columns in our survey dataframe.\nUse lapply() to apply pet_split() to all of the \u0026lt;DRINK\u0026gt;_days columns in our survey dataframe. Save the results as drink_dfs. DO NOT include the () after pet_split when providing it as an argument; it will produce an error.\nYou will have gotten a list object of length 5 back. Recall that lists are super-vectors. In this case, each element of our list contains an entire dataframe!\nWorking with Lists Working with lists is tricky. Because they can contain anything it is hard to have set rules. The only one you can always rely on is that you need to use double square brackets [[ ]] to work with the contents of a list. For example, in our new drink_dfs list, we can ask for the content of the first element either by position (drink_dfs[[1]]) or by name (drink_dfs[[\u0026quot;coffee_days\u0026quot;]]). If you only use a single square bracket with a list (drink_dfs[1]) you will get the actual first element of a list, which is a list of length 1 with whatever the contents are inside of it. Confusing, I know.\nRun str() on both of the following:\ndrink_dfs[[1]] drink_dfs[1] What are the differences between the two?\nOnce we subset the list using [[ ]], we can treat it like it was a normal dataframe; auto-complete will even still work! Start typing drink_dfs[[\u0026quot;coffee_days\u0026quot;]]$ into the console and hit tab. All of the columns of that dataframe will appear as normal!\nA list of dataframes is actually a handy structure though, because it will allow us to work with all of those dataframes simultaneously. In the following example, I use lapply() to perform a custom function on each of our drink dataframes all at once. First I subset to just the day columns, then a get the column sums for each day.\nlapply(drink_dfs, FUN = function(drink){ # for each drink, find what day is was most frequently consumed by our class # subset to the day columns days = drink[, c(\u0026#34;monday\u0026#34;, \u0026#34;tuesday\u0026#34;, \u0026#34;wednesday\u0026#34;, \u0026#34;thursday\u0026#34;, \u0026#34;friday\u0026#34;, \u0026#34;saturday\u0026#34;, \u0026#34;sunday\u0026#34;)] # get the column sums for each day # recall each row is a person in class, and each column is a day # so the column sums would be the number of people who had X drink per day class_consumed = colSums(days) # return output return(class_consumed) }) $coffee_days monday tuesday wednesday thursday friday saturday sunday 4 5 5 2 2 3 5 $tea_days monday tuesday wednesday thursday friday saturday sunday 0 0 2 0 1 1 3 $soda.pop_days monday tuesday wednesday thursday friday saturday sunday 4 4 2 2 3 1 3 $juice_days monday tuesday wednesday thursday friday saturday sunday 6 6 7 7 8 6 5 $none_days monday tuesday wednesday thursday friday saturday sunday 4 3 1 4 2 2 2 That would have taken so much more typing otherwise. An apply function like this can make our code much cleaner and easier to read. If we did have to type all that out, it also increases the chance we will make a mistake somewhere, and one of our calculations will be different from the others. This way, we know all of them are exactly the same. You could also accomplish the same thing with a for() loop, and there are rather minimal differences in most cases. Do whatever makes most sense to you!\nRead ALL the Files! I promised I would share how to read multiple files at once and here we are. This is actually a common way I use lapply(). First, we need to have a directory of data files with identical structures. For this example, we will use the data from lab 3 (aggregation and merging). First, specify the path to the data folder of lab 3. You have to find the specific path on your computer.\n# change this to match your system lab_3_data = \u0026#34;path/to/lab-3-tidy-agg-merge-NAME/data/\u0026#34; Next, we will get a vector of all the file paths of the data inside that folder using list.files(). I use the pattern argument here to say \u0026ldquo;I only want files that contain this.\u0026rdquo; In this case, it is the \u0026ldquo;econ_\u0026rdquo; prefix so I only get our \u0026ldquo;econ_acs5_YEAR.csv\u0026rdquo; files.\necon_data_paths = list.files(lab_3_data, pattern = \u0026#34;econ_\u0026#34;, full.names = TRUE) Now, we will use lapply() to read in all of the econ_X dataframes from that lab at once.\nall_econ_data = lapply(econ_data_paths, read.csv) You now have a list of length 6 with all of the econ data from that lab! No copy and pasting required. You can even pivot and merge them all at once too. First, I\u0026rsquo;ll pivot everything from long to wide:\nlibrary(tidyr) all_econ_data_wide = lapply(all_econ_data, FUN = pivot_wider, id_cols = c(\u0026#34;GEOID\u0026#34;, \u0026#34;NAME\u0026#34;), names_from = \u0026#34;variable\u0026#34;, values_from = c(\u0026#34;estimate\u0026#34;, \u0026#34;moe\u0026#34;)) Next, we\u0026rsquo;ll use basename() to give each dataframe a year identifier (we\u0026rsquo;ll just use the file name for now). I\u0026rsquo;ll do this one in a for() loop as it\u0026rsquo;s easier to match the file names vector and our data list elements.\nfor(i in 1:6){ # get the file name I want file_name = basename(econ_data_paths[i]) # add that as a column to the matching list element all_econ_data_wide[[i]]$file_name = file_name } Now we can bind them all together. I\u0026rsquo;ll use do.call() here. Really this is the only time I ever use it, and I don\u0026rsquo;t know what else it is helpful for.\nmerged_econ = do.call(rbind, all_econ_data_wide) Repeat this process with the \u0026ldquo;pop_acs5_XXXX\u0026rdquo; CSVs from lab 3.\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/class_worksheets/23_bash/","title":"Bash","tags":[],"description":"","content":" Overview Get the Game Playing the Game Overview Today\u0026rsquo;s spooky worksheet will (hopefully) be a bit of a fun divergence. To get comfortable navigating around the terminal, we will be playing a video game, GameShell! Or rather the venerable ancestor to modern video games, the text based role playing game. You\u0026rsquo;ll be delving into the depths of castle cellars and slaying spiders like a proper Halloween adventure!\nFor this worksheet, please complete all the basic missions, #1 through #12. Feel free to adventure more though!\nGet the Game Fittingly for a terminal based game, you\u0026rsquo;ll actually download the game through the terminal! We need to do some prep work first though. To start off, open up a terminal and copy \u0026amp; paste the following command. It will ask you to enter your user password because it is installing new software for the game to run. You will not see your password as you enter it, this is normal.\nsudo apt install gettext man-db procps psmisc nano tree bsdmainutils x11-apps wget If you want to know what this command does:\nsudo (super-user do) elevates this command to admin status so it can install new things, that\u0026rsquo;s why you need the password apt install tells the terminal to install the following programs The rest are programs, most of which you probably already have (gettext man-db procps psmisc nano tree bsdmainutils x11-apps wget) Next up, you will use wget or web-get to download the game file off the internet. It will save the file wherever you currently are in the terminal, so probably best to move back to your user directory if you have moved elsewhere.\nwget https://github.com/phyver/GameShell/releases/download/latest/gameshell.sh Playing the Game Once both of those commands are finished running, you can enter the following to start the game:\nbash gameshell.sh Once you start the game, it will provide instructions on how to proceed. You will be using many of the commands we learned today. There are a few game specific commands that it will ask you to use. Know that these are just for the game, they won\u0026rsquo;t work anywhere else:\ngsh goal Check your current goal gsh check Check if you have completed your current goal gsh help Get help with game specific commands "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/hidden/radiation_vid/","title":"Content Warning - How history&#39;s worst software error weaponized a radiation machine","tags":[],"description":"","content":"This video contains content that me be distressing to some. It covers topics related to death and cancer, and shows some images of radiation damage on the human body. While gruesome, these topics are important to discuss, given it was a failure in robust coding that caused these tragedies.\nLink to the Perusall Page\n"},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/hidden/","title":"Hidden","tags":[],"description":"","content":""},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/credits/","title":"Credits","tags":[],"description":"","content":"Content Thank you to Dr. Lindsay Poirier and Dr. Ben Baumner for providing their materials as reference as I prepared this course reader.\nThanks to the UC Davis DataLab for providing materials for the install guides.\nBackend Thanks to the Hugo projects for providing the infrastructure for this site. Thank you to the team that created Hugo Learn Theme, the basis of this reader.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/","title":"Intro to Data Science","tags":[],"description":"","content":" Intro to Data Science Quick Links Resource Link Description Syllabus Quick access to all important course information. Moodle Grades and quizzes will be available on the course Moodle. Slack Main communication channel for the course. Perusall Perusall page for course readings Spinelli Center The Spinelli Center offers drop-in tutoring hours in Sabin-Reed 301 or on Zoom. Office Hours Sign up for a slot in office hours here. Overview Info Value Who Dr. Jared Joseph What SDS 192-03: Introduction to Data Science When Mondays 1:40-2:55pm; Wednesday/Friday 1:20-2:35pm Where Stoddard G2 Schedule Below is the tentative schedule for the course. While we will try to keep to this schedule, unanticipated situations (and mountain day) may require us to adjust. Each row is a class meeting, with the readings and assignments due on that day listed.\nWeek Date Topic Readings Due 1 9/5/2022 (Mon) No Class 1 9/7/2022 (Wed) Introduction 1 9/9/2022 (Fri) What is Data? Class Syllabus Kitchin, R., \u0026amp; Lauriault, T. P. (2018). Toward Critical Data Studies: Charting and Unpacking Data Assemblages and Their Work. in J. Thatcher, J. Eckert, \u0026amp; A. Shears (Eds.), Thinking Big Data in Geography: New Regimes, New Research (pp. 3-20). University of Nebraska Press. Welcome Survey Data Survey Install Slack and join the class workspace 2 9/12/2022 (Mon) Install Day 2 9/14/2022 (Wed) Intro to R/R Studio (Posit) Irizarry, R. A. (2022). Chapter 2 R basics | Introduction to Data Science. In Introduction to Data Science. All Software Installed 2 9/16/2022 (Fri) Lab 0 \u0026amp; LAB 1 3 9/19/2022 (Mon) Intro to git/GitHub Bryan, J. (2018). Excuse Me, Do You Have a Moment to Talk About Version Control? American Statistician, 72(1), 20-27. 3 9/21/2022 (Wed) Exploratory Data Analyses Irizarry, R. A. (2022). Chapter 12 Robust summaries. In Introduction to Data Science. [PAGES 3-12] Grant, R. (2019). Why visualize? in Data Visualization: Charts, Maps, and Interactive Graphics. Chapman and Hall/CRC. Holtz, Y., \u0026amp; Healy, C. (2018). The issue with pie chart in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). Venn Diagram in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). Line chart in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). Barplot in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). Scatter plot in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). Histogram in From data to Viz. Holtz, Y., \u0026amp; Healy, C. (2018). The Boxplot and its pitfalls in From data to Viz. Lab 0 Lab 1 3 9/23/2022 (Fri) LAB 2 4 9/26/2022 (Mon) Tidy Data/Long-Wide [Section 6.1-6.3] Baumer, B. S., Kaplan, D. T., \u0026amp; Horton, N. J. (2021). Chapter 6 Tidy data. In Modern Data Science with R. CNC Press. 4 9/28/2022 (Wed) Aggregation and Merging Ismay, C., \u0026amp; Kim, A. Y. (2022). Chapter 3 Data Wrangling. In Statistical Inference via Data Science. CNC Press. [PAGES 1701-1731] Ohm, P. (2009). Broken Promises of Privacy: Responding to the Surprising Failure of Anonymization (SSRN Scholarly Paper No. 1450006). Lab 2 4 9/30/2022 (Fri) LAB 3/Quiz 1 Open 5 10/3/2022 (Mon) Advanced Plotting Reynolds, P. (2021). 5 Principles of Visual Perception in Principles of Data Visualization. Irizarry, R. A. (2022). Chapter 8 ggplot2 | Introduction to Data Science. In Introduction to Data Science. Leo, S. (2019, March 27). Mistakes, weve drawn a few. Medium. 5 10/5/2022 (Wed) Dynamic Plotting Scroll through Spurious Correlations Explore U.S. Gun Deaths Sievert, C. (2019). 1 Preface. In Interactive web-based data visualization with R, plotly, and shiny. Holtz, Y. (2018). Interactive charts | the R Graph Gallery. Lab 3 5 10/7/2022 (Fri) LAB 4 Quiz 1 6 10/10/2022 (Mon) No Class 6 10/12/2022 (Wed) Data Science Ethics Baumer, B. S., Kaplan, D. T., \u0026amp; Horton, N. J. (2021). Chapter 8 Data science ethics. In Modern Data Science with R. CNC Press. Lab 4 6 10/14/2022 (Fri) Project 1 7 10/17/2022 (Mon) Functions Grolemund, G., \u0026amp; Wickham, H. (2017). 19 Functions. In R for Data Science. O'Reilly. Kyle Hill (Director). (2022, August 31). How history's worst software error weaponized a radiation machine. 7 10/19/2022 (Wed) R Debugging \u0026amp; Conditions Bryan, J., \u0026amp; Hester, J. (2021). Chapter 11 Debugging R code. In What They Forgot to Teach You About R. 7 10/21/2022 (Fri) LAB 5 8 10/24/2022 (Mon) Iteration Wickham, H., \u0026amp; Grolemund, G. (2017). 21 Iteration. In R for Data Science. O'Reilly. 8 10/26/2022 (Wed) Lists and Apply Peng, R. D. (2022). 24 Parallel Computation. In R Programming for Data Science. Lab 5 8 10/28/2022 (Fri) LAB 6/Quiz 2 Open Project 1 9 10/31/2022 (Mon) Bash Irizarry, R. A. (2022). Chapter 39 Organizing with Unix. In Introduction to Data Science. 9 11/2/2022 (Wed) Advanced git/GitHub Turing Way Community. (2022). Git Branches. In The Turing Way: A handbook for reproducible, ethical and collaborative research. Zenodo. Turing Way Community. (2022). Merging Branches in Git. In The Turing Way: A handbook for reproducible, ethical and collaborative research. Zenodo. Turing Way Community. (2022). Retrieving and Comparing Versions. In The Turing Way: A handbook for reproducible, ethical and collaborative research. Zenodo. Lab 6 9 11/4/2022 (Fri) LAB 7 Quiz 2 10 11/7/2022 (Mon) Data Cleaning de Jonge, E., \u0026amp; van der Loo, M. (2013). An introduction to data cleaning with R. Rue, J., \u0026amp; Hernandez, R. K. (2019). Using OpenRefine to Clean Your Data. Berkeley Advanced Media Institute. Farivar, C. (2016, August 10). Kansas couple sues IP mapping firm for turning their life into a 'digital hell.' Ars Technica. 10 11/9/2022 (Wed) Recap Lab 7 10 11/11/2022 (Fri) Project 2 11 11/14/2022 (Mon) Web Scraping Irizarry, R. A. (2022). Chapter 24 Web scraping. In Introduction to Data Science. Zimmer, M. (2010). 'But the data is already public': On the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313-325. 11 11/16/2022 (Wed) Remote Servers \u0026amp; APIs TBD 11 11/18/2022 (Fri) LAB 8/Quiz 3 Open 12 11/21/2022 (Mon) SQL TBD 12 11/23/2022 (Wed) No Class Lab 8 12 11/25/2022 (Fri) No Class Project 2 \u0026amp; Quiz 3 13 11/28/2022 (Mon) Finals Planning Final Project Ideas 13 11/30/2022 (Wed) Text as Data Clark, M. (2018). String Theory. In An Introduction to Text Processing and Analysis with R. Peng, R. D. (2022). 17 Regular Expressions. In R Programming for Data Science. 13 12/2/2022 (Fri) Networks as Data/Quiz 4 Open [Chapters 1-2] Kadushin, C. (2012). Understanding Social Networks: Theories, Concepts, and Findings. Oxford University Press. Berman, G. (2021, November 31). 'Violence Is Contagious': A Conversation with Andrew Papachristos. Harry Frank Guggenheim Foundation. 14 12/5/2022 (Mon) Geospatial Data Baumer, B. S., Kaplan, D. T., \u0026amp; Horton, N. J. (2021). Chapter 17 Working with geospatial data. In Modern Data Science with R. CNC Press. de Montjoye, Y.-A., Hidalgo, C. A., Verleysen, M., \u0026amp; Blondel, V. D. (2013). Unique in the Crowd: The privacy bounds of human mobility. Scientific Reports, 3(1), 1376. Deluca, E., \u0026amp; Nelson, S. (2017). 7. Lying With Maps. In Mapping, Society, and Technology. University of Minnesota Libraries Publishing. 14 12/7/2022 (Wed) Mountain Day Saftey Net/Lessons Learned 14 12/9/2022 (Fri) Finals Work Quiz 4 15 12/12/2022 (Mon) Finals Presentations/aRt Gallery 15 12/14/2022 (Wed) No Class Final Project Materials "},{"uri":"https://intro-to-data-science-template.github.io/intro_to_data_science_reader/tags/","title":"Tags","tags":[],"description":"","content":""}]